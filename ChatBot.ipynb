{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1wWc0yqx4cu4YaLKX8wwbXpSlEhffyms5",
      "authorship_tag": "ABX9TyOPPOEdepv0SLxhKLelle5z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drbarak/NLP-FinalProject/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB2riNzw6QD6"
      },
      "source": [
        "INSTALL = False\n",
        "if INSTALL:\n",
        "  # to switch version of spacy\n",
        "  !pip uninstall spacy\n",
        "  !pip uninstall en_core_web_mdy\n",
        "  \n",
        "  !pip install spacy==2.2.4 #3.0.6\n",
        "\n",
        "  !pip install pyforest\n",
        "  #!pip install wget\n",
        "  !python -m spacy download en_core_web_md"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UENbsDfWvtF2"
      },
      "source": [
        "# INIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_gXeiB58h8k"
      },
      "source": [
        "from importlib import import_module\n",
        "p = print\n",
        "def my_import(lib, package=None, func=None, version=None):\n",
        "  try:\n",
        "    import_module(lib)\n",
        "  except:\n",
        "    if package is None: package = lib\n",
        "    if version:\n",
        "      p('pip install \"{package}=={version}\"')\n",
        "      !pip install \"{package}=={version}\"\n",
        "    else:\n",
        "      p('pip install \"{package}\"')\n",
        "      !pip install \"{package}\"\n",
        "    import_module(lib)\n",
        "  \n",
        "  if func is not None:\n",
        "    return import_module(lib).__getattribute__(func)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgpLToDvAC0z"
      },
      "source": [
        "my_import('pyforest')\n",
        "import json, bs4\n",
        "import string\n",
        "import random\n",
        "import time, pytz\n",
        "from datetime import datetime\n",
        "tz = pytz.timezone('Israel')\n",
        "\n",
        "ic = my_import('icecream', func='ic')\n",
        "ic.configureOutput(includeContext=True)  # include module name and line number\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "space_punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
        "\n",
        "p = print\n",
        "d = display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opt4fUcLfvZ_"
      },
      "source": [
        "#my_import('spacy', version='3.0.6')\n",
        "p('spacy.__version__', spacy.__version__)\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.language import Language\n",
        "\n",
        "# Load md (medium) English tokenizer, tagger, parser and NER, and vectors(vectors are not included in the 'sm' model)\n",
        "#nlp = spacy.load('en_core_web_md')  NOT WORKING ON COLAB\n",
        "try:\n",
        "  import en_core_web_md\n",
        "except:  \n",
        "  !python -m spacy download en_core_web_md\n",
        "  import en_core_web_md\n",
        "  \n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OiSqiCC9wy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2eda3ce7-c3d3-4140-ee9e-790f6daee2c5"
      },
      "source": [
        "nltk.download(['punkt', 'stopwords'])\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z58HiRJLMen"
      },
      "source": [
        "#!pip uninstall googletrans\n",
        "my_import('googletrans', version='3.1.0a0')\n",
        "from googletrans import Translator\n",
        "translator = Translator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdnCbXKo5cwo"
      },
      "source": [
        "import requests\n",
        "\n",
        "api_key = 'c2adfa29edfd95ad16efab9218619ff3'\n",
        "URL = \"http://api.openweathermap.org/data/2.5/{0}?\"\n",
        "icon_url = \" http://openweathermap.org/img/wn/{0}@2x.png\"  # 10d\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e8DgeWTF03z"
      },
      "source": [
        "path = '/content/drive/MyDrive/Project5_ChatBot/'\n",
        "data_url = 'https://raw.githubusercontent.com/DanielKorenDataScience/NLP-FinalProject/main/data/' "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFqfk8lMh8x-"
      },
      "source": [
        "TEST = False\n",
        "LANG = 'en'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJpv2GTU2-l7"
      },
      "source": [
        "def process_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    result = []\n",
        "    for token in doc:\n",
        "        if token.text in nlp.Defaults.stop_words:   # len(nlp.Defaults.stop_words)=326\n",
        "            continue\n",
        "        if token.is_punct:\n",
        "            continue\n",
        "        if token.lemma_ == '-PRON-':\n",
        "            ic('-PRON-: ', token.lemma_)\n",
        "            continue\n",
        "        result.append(token.lemma_)\n",
        "    return \" \".join(result)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG99z-tM4cl4"
      },
      "source": [
        "def calculate_similarity(text1, text2):\n",
        "    def get_similarity(doc1, doc2):\n",
        "      if doc1 and doc1.vector_norm:\n",
        "        if doc2 and doc2.vector_norm:\n",
        "          return doc1.similarity(doc2)\n",
        "      return 0\n",
        "\n",
        "    doc1 = nlp(text1)\n",
        "    #ic(doc1.pos_)\n",
        "    doc2 = nlp(text2)\n",
        "    similarity1 = get_similarity(doc1, doc2)\n",
        "\n",
        "    doc1 = nlp(process_text(text1))\n",
        "    #ic(doc1)\n",
        "    doc2 = nlp(process_text(text2))\n",
        "    #ic(doc2)\n",
        "    similarity2 = get_similarity(doc1, doc2)\n",
        "\n",
        "    return max(similarity1, similarity2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMXfN0EnkL3a"
      },
      "source": [
        "# Get cities and update model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF2CJIu3pOik"
      },
      "source": [
        "GIT = True\n",
        "git_path = 'https://raw.githubusercontent.com/DanielKorenDataScience/NLP-FinalProject/main/data/'\n",
        "\n",
        "def get_json(fname, GIT=True):\n",
        "  p(fname)\n",
        "  if GIT:\n",
        "    resp = requests.get(data_url + fname)\n",
        "    return json.loads(resp.text)\n",
        "  with open(path + fname, encoding=\"utf8\") as f:\n",
        "    return json.loads(f.read())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "W1gHA85O9isU",
        "outputId": "375292e3-913c-4c25-908d-8c45fd6b2911"
      },
      "source": [
        "def load_cities_il():\n",
        "  path_ = path\n",
        "  fname = 'cities_IL.csv'\n",
        "  if False and GIT:\n",
        "    path_ = git_path\n",
        "  df = pd.read_csv(path_ + fname, names=['city', 'district']) # the 'district' are recognized by the api\n",
        "  df.city = df.city.str.translate(remove_punct_dict)\n",
        "  CITIES = df.city.tolist()\n",
        "  df = df.set_index('city')\n",
        "  return df, CITIES\n",
        "cities_il_df, CITIES_IL = load_cities_il()\n",
        "cities_il_df.tail()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>district</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>city</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>galil</th>\n",
              "      <td>northern district</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sharon</th>\n",
              "      <td>netanya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>north</th>\n",
              "      <td>northern district</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>central</th>\n",
              "      <td>netanya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>center</th>\n",
              "      <td>netanya</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  district\n",
              "city                      \n",
              "galil    northern district\n",
              "sharon             netanya\n",
              "north    northern district\n",
              "central            netanya\n",
              "center             netanya"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JeLKymSl2Rb"
      },
      "source": [
        "UPD_IL = False\n",
        "if UPD_IL:\n",
        "  #CITIES_API = get_json('city.list.json')\n",
        "  CITIES_API = CITIES_API[:10]\n",
        "\n",
        "  df = pd.DataFrame(CITIES_API)\n",
        "  df = df.drop(['id'], axis='columns')\n",
        "  df.name = df.name.str.lower()\n",
        "  df.name = df.name.str.translate(remove_punct_dict)\n",
        "\n",
        "  CITIES_API_city = sorted(set(df.name.to_list()))\n",
        "  p(CITIES_API_city)\n",
        "\n",
        "  #p(CITIES_API.values())\n",
        "  #for c in CITIES_API\n",
        "  'Ḩeşār-e Sefīd'.lower().translate(remove_punct_dict) in CITIES_API_city#CITIES_API[0].values()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "jVyPlh34vAJ8",
        "outputId": "2297a07e-c1ae-44ac-b3e8-374bb3b2a8c0"
      },
      "source": [
        "#my_import('wget')\n",
        "#import wget\n",
        "\n",
        "def load_CITIES_API():\n",
        "  # file too big for git so read it from my google drive using a shared link\n",
        "  CITIES_API = get_json('city.list.json')\n",
        "  ''' \n",
        "  git_path = 'https://raw.githubusercontent.com/DanielKorenDataScience/NLP-FinalProject/main/' + 'city.list.json'\n",
        "  resp = requests.get(git_path)\n",
        "  CITIES_API = json.loads(resp.text)\n",
        "\n",
        "  google_shared_link = 'https://drive.google.com/file/d/19lB4T32o1VbFAKRk_PExbNYV8mcZ8hRT/view?usp=sharing'\n",
        "  url = 'https://drive.google.com/uc?authuser=0&id=19lB4T32o1VbFAKRk_PExbNYV8mcZ8hRT&export=download'\n",
        "  file_id = '19lB4T32o1VbFAKRk_PExbNYV8mcZ8hRT'\n",
        "  \n",
        "  while True:\n",
        "    try:\n",
        "      with open('city.list.json', encoding=\"utf8\") as f:\n",
        "        CITIES_API = json.loads(f.read())\n",
        "      break\n",
        "    except:\n",
        "      wget.download(url)  # download the file to local google drive with the original name in the share link\n",
        "      # now can open with no path because it is in the local 'colab' directory\n",
        "  \n",
        "  '''\n",
        "  # add all israeli cities so when looking for 'north' gets multiple cities because there is one in France\n",
        "  # and not to get an error is specify 'north, israel' becuse the country does not match\n",
        "  df = pd.DataFrame(CITIES_API)\n",
        "  df.name = df.name.str.lower()\n",
        "  df.name = df.name.str.translate(remove_punct_dict)\n",
        "  CITIES_API_city = sorted(set(df.name.to_list()))\n",
        "\n",
        "  for city in CITIES_IL:\n",
        "    CITIES_API.append({'name': city, 'country': 'IL', 'state': '', 'coord': ''})\n",
        "  \n",
        "  df = pd.DataFrame(CITIES_API)\n",
        "  df.name = df.name.str.lower()\n",
        "  df.name = df.name.str.translate(remove_punct_dict)\n",
        "  df = df.drop(['id'], axis='columns')\n",
        "\n",
        "  CITIES_API_city = sorted(set(df.name.to_list()))\n",
        "  CITIES_API_country = sorted(set(df.country.str.lower().to_list()))\n",
        "  if CITIES_API_country[0] == '':\n",
        "    CITIES_API_country = CITIES_API_country[1:]\n",
        "  CITIES_API_state = sorted(set(df.state.str.lower().to_list()))\n",
        "  if CITIES_API_state[0] == '':\n",
        "    CITIES_API_state = CITIES_API_state[1:]\n",
        "  if CITIES_API_state[0] == '00':\n",
        "    CITIES_API_state = CITIES_API_state[1:]\n",
        "  \n",
        "  df.set_index('name', inplace=True)\n",
        "  #d(df.head)\n",
        "  return df, CITIES_API_city, CITIES_API_country, CITIES_API_state\n",
        "\n",
        "df_CITIES_API, CITIES_API_city, CITIES_API_country_code, CITIES_API_state_code = load_CITIES_API()\n",
        "df_CITIES_API.head()\n",
        "#df_CITIES_API.loc['taglag'].coord"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "city.list.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "      <th>coord</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ḩeşāre sefīd</th>\n",
              "      <td></td>\n",
              "      <td>IR</td>\n",
              "      <td>{'lon': 47.159401, 'lat': 34.330502}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‘ayn ḩalāqīm</th>\n",
              "      <td></td>\n",
              "      <td>SY</td>\n",
              "      <td>{'lon': 36.321911, 'lat': 34.940079}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>taglag</th>\n",
              "      <td></td>\n",
              "      <td>IR</td>\n",
              "      <td>{'lon': 44.98333, 'lat': 38.450001}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qabāghlū</th>\n",
              "      <td></td>\n",
              "      <td>IR</td>\n",
              "      <td>{'lon': 46.168499, 'lat': 36.173302}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‘arīqah</th>\n",
              "      <td></td>\n",
              "      <td>SY</td>\n",
              "      <td>{'lon': 36.48336, 'lat': 32.889809}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             state country                                 coord\n",
              "name                                                            \n",
              "ḩeşāre sefīd            IR  {'lon': 47.159401, 'lat': 34.330502}\n",
              "‘ayn ḩalāqīm            SY  {'lon': 36.321911, 'lat': 34.940079}\n",
              "taglag                  IR   {'lon': 44.98333, 'lat': 38.450001}\n",
              "qabāghlū                IR  {'lon': 46.168499, 'lat': 36.173302}\n",
              "‘arīqah                 SY   {'lon': 36.48336, 'lat': 32.889809}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "4u5cXlHgxKsF",
        "outputId": "db5ee901-270b-47bf-be7d-7421c184d77c"
      },
      "source": [
        "def load_countries():\n",
        "  path_ = path  \n",
        "  fname = 'countries.csv'\n",
        "  if GIT:\n",
        "    path_ = git_path\n",
        "  df = pd.read_csv(path_ + fname)\n",
        "  df.Name = df.Name.str.lower()\n",
        "  COUNTRIES = df.Name.tolist()\n",
        "  df = df.set_index('Code')\n",
        "  return df, COUNTRIES\n",
        "\n",
        "countries_df, COUNTRIES = load_countries()\n",
        "countries_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Code</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AF</th>\n",
              "      <td>afghanistan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AX</th>\n",
              "      <td>åland islands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AL</th>\n",
              "      <td>albania</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DZ</th>\n",
              "      <td>algeria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AS</th>\n",
              "      <td>american samoa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Name\n",
              "Code                \n",
              "AF       afghanistan\n",
              "AX     åland islands\n",
              "AL           albania\n",
              "DZ           algeria\n",
              "AS    american samoa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "J-gKXG1hHu4J",
        "outputId": "533d13ce-7bc9-40f9-edb4-a640be48d187"
      },
      "source": [
        "def load_capitals():\n",
        "  path_ = path\n",
        "  fname = 'capitals.json'\n",
        "  if GIT:\n",
        "    path_ = git_path\n",
        "  CAPITALS = pd.read_json(path_ + fname, encoding=\"utf8\", typ='series').str.lower()\n",
        "\n",
        "  CAPITALS.index = CAPITALS.index.str.lower()\n",
        "  df = pd.DataFrame([CAPITALS])\n",
        "  df = df.T\n",
        "  df = df.reset_index()\n",
        "  df.columns = ['country', 'capital']\n",
        "  df = df.set_index('capital')\n",
        "  return df, CAPITALS.to_dict()\n",
        "capitals_df, CAPITALS = load_capitals()\n",
        "\n",
        "p(CAPITALS['albania'])\n",
        "p(capitals_df.loc['madrid'].country)\n",
        "capitals_df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tirana\n",
            "spain\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>capital</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>kabul</th>\n",
              "      <td>afghanistan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mariehamn</th>\n",
              "      <td>åland islands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tirana</th>\n",
              "      <td>albania</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>algiers</th>\n",
              "      <td>algeria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pago pago</th>\n",
              "      <td>american samoa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  country\n",
              "capital                  \n",
              "kabul         afghanistan\n",
              "mariehamn   åland islands\n",
              "tirana            albania\n",
              "algiers           algeria\n",
              "pago pago  american samoa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "1YMQgZMecwVh",
        "outputId": "00eed139-792d-4e5e-e308-9b607de20f3e"
      },
      "source": [
        "def load_largest():\n",
        "  fname = '100-largest-cities.csv'\n",
        "  path_ = path\n",
        "  if GIT:\n",
        "    path_ = git_path\n",
        "  df = pd.read_csv(path_ + fname, encoding='ISO-8859-8')\n",
        "  df.city = df.city.str.lower()\n",
        "  df.city = df.city.str.translate(remove_punct_dict)\n",
        "  LARGEST = df.city.tolist()\n",
        "  df.country = df.country.str.lower()\n",
        "  df = df.set_index('city')\n",
        "  return df, LARGEST\n",
        "largest_df, LARGEST = load_largest()\n",
        "\n",
        "d(largest_df.head())\n",
        "p(largest_df.loc['toronto'].country)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>city</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tokyo</th>\n",
              "      <td>japan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delhi</th>\n",
              "      <td>india</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shanghai</th>\n",
              "      <td>china</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>so paulo</th>\n",
              "      <td>brazil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ciudad de mxico mexico city</th>\n",
              "      <td>mexico</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            country\n",
              "city                               \n",
              "tokyo                         japan\n",
              "delhi                         india\n",
              "shanghai                      china\n",
              "so paulo                     brazil\n",
              "ciudad de mxico mexico city  mexico"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "canada\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ForSoWmMl9il",
        "outputId": "541db945-5ca6-40f5-d41b-84bee9ad1c62"
      },
      "source": [
        "def load_actions():\n",
        "  path_ = path\n",
        "  fname = 'actions.json'\n",
        "  actions = get_json(fname, False)\n",
        "  ACTIONS_patterns_ = [y for tag in actions[\"actions\"] for y in tag['patterns']]\n",
        "  ACTIONS_ = {y: tag['tag'] for tag in actions[\"actions\"] for y in tag['patterns']}\n",
        "  ACTIONS_tags_ = {tag['tag']:'' for tag in actions[\"actions\"]}\n",
        "  return ACTIONS_patterns_, ACTIONS_, ACTIONS_tags_\n",
        "ACTIONS_patterns, ACTIONS, ACTIONS_tags = load_actions()\n",
        "\n",
        "p(ACTIONS_patterns[-5:])\n",
        "p(list(ACTIONS.items())[:5])\n",
        "p(ACTIONS.keys())\n",
        "p(ACTIONS_tags)\n",
        "ACTIONS['hot']\n",
        "ACTIONS_tags.keys()\n",
        "'clouds' in ACTIONS"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actions.json\n",
            "['evening', 'sundown', 'cloud', 'clouds', 'fahrenheit']\n",
            "[('weather', 'all'), ('tornedo', 'weatherWind'), ('hurricane', 'weatherWind'), ('gale', 'weatherWind'), ('gust', 'weatherWind')]\n",
            "dict_keys(['weather', 'tornedo', 'hurricane', 'gale', 'gust', 'whirlwind', 'cyclone', 'fog', 'mist', 'isobar', 'barometric', 'heat wave', 'freeze', 'climate', 'conditions', 'forecast', 'outlook', 'thunder', 'thunderstorm', 'storm', 'rain', 'drizzle', 'snow', 'dry', 'rainbow', 'overcast', 'shower', 'lightning', 'blustery', 'downpour', 'tropical', 'water cycle', 'drought', 'hail', 'icicle', 'flood', 'muggy', 'flash flood', 'atmospheric', 'cold front', 'cold snap', 'condensation', 'ice storm', 'snowfall', 'air', 'balmy', 'avalanche', 'frost', 'blizzard', 'spring', 'smog', 'ozone', 'autumn', 'winter', 'summer', 'sleet', 'sky', 'easterlies', 'westerlies', 'dew', 'surge', 'monsoon', 'permafrost', 'temperature', 'hot', 'warn', 'cold', 'temp', 'degree', 'thermal', 'heat', 'heat index', 'warm', 'pressure', 'smoke', 'haze', 'dust', 'sand', 'humidity', 'damp', 'moist', 'moisture', 'visibility', 'wind', 'breeze', 'draft', 'blow', 'sunrise', 'sunset', 'dawn', 'daybreak', 'night', 'dark', 'first light', 'morning', 'nightfall', 'twilight', 'dusk', 'evening', 'sundown', 'cloud', 'clouds', 'fahrenheit'])\n",
            "{'all': '', 'weatherWind': '', 'weatherPressure': '', 'weatherTemp': '', 'weather': '', 'temp': '', 'pressure': '', 'humidity': '', 'visibility': '', 'wind': '', 'sunrise': '', 'clouds': '', 'fahrenheit': ''}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "WHbmjgetc318",
        "outputId": "da789900-fd6a-4644-d6d7-83d1d0fd5c82"
      },
      "source": [
        "def load_test():\n",
        "  path_ = path\n",
        "  fname = 'test_questions.txt'\n",
        "  questions = get_json(fname, False)\n",
        "  return questions\n",
        "QUESTIONS = load_test()\n",
        "\n",
        "p(QUESTIONS)\n",
        "QUESTIONS[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_questions.txt\n",
            "['Weather forecast in Tel aviv', \"'קשאיקר ןמ ןדרשקך\", 'How hot is it in Herzliya', 'Please send the log to drbarak@talkie.co.il', 'Today’s temperature in Netanya', 'Moist in Hadera', 'How many degrees tonight in Hadera Ofakim', 'Weekly forecast for Ramat Hasharon', 'Israel desert forecast', 'Weather in Ramat gan', 'Moist in Givatayim', 'Wave height in Ashdod', 'Moist ashkelon', 'Night degrees in Eilat', 'Newt 10 days forecast in Herzliya and Raanana', 'Is there a weather forecast on Herzliya', 'Would like to get weather forecast on Hadera next week', 'North Israel weather', 'Monthly average forecast for north israel', 'Monthly average forecast on Thailand', 'What are the optional cities for weather forecast', 'Who is the first prime minister of Israel', 'Who where the 3 stooges', 'What is a banana', 'Who was Napoleon', 'When was photography invented', 'Are there aliens']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Weather forecast in Tel aviv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orltdBSKZ2uR"
      },
      "source": [
        "dayInWeek={'sunday':1,'monday':2,'tuesday':3,'wednesday':4,'thursday':5,'friday':6,'saturday':7}\n",
        "dateToNum={\"today\":0,\"tomorrow\":1,'twoDays':2 }\n",
        "textToNumbers={'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7}"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "cCc1oYzyX-Bo",
        "outputId": "7b06ebd3-16d1-4e96-f489-b0afae943e56"
      },
      "source": [
        "def load_dates():\n",
        "  fname = 'datesWords.txt'\n",
        "  path_ = path\n",
        "  if False and GIT:\n",
        "    path_ = data_url\n",
        "    print(path_)\n",
        "  df = pd.read_csv(path_ + fname)\n",
        "  #a_series = pd. Series(['','','','in 1 week'], index = df. columns)\n",
        "  #df = df.append(a_series, ignore_index=True)\n",
        "  df.fillna('', inplace=True)\n",
        "  return df\n",
        "datesDf= load_dates()\n",
        "listNlp = {}\n",
        "for column in datesDf:\n",
        "  listNlp.update({column: [nlp.make_doc(t) for t in datesDf[column].tolist() if t != '']})\n",
        "#p(listNlp)\n",
        "datesDf"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk\\nimport bokeh\\nimport fbprophet\\nimport pydot\\nfrom sklearn.model_selection import cross_val_score\\nimport pandas as pd\\nfrom xlrd import open_workbook\\nimport os\\nimport plotly as py\\nimport pickle\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport numpy as np\\nimport altair as alt\\nfrom sklearn.manifold import TSNE\\nimport imutils\\nimport spacy\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport re\\nimport fastai\\nfrom statsmodels.tsa.arima_model import ARIMA\\nimport plotly.express as px\\nimport matplotlib.pyplot as plt\\nimport cv2'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>today</th>\n",
              "      <th>tomorrow</th>\n",
              "      <th>twoDays</th>\n",
              "      <th>thisweek</th>\n",
              "      <th>nextWeek</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>now</td>\n",
              "      <td>tomorrow</td>\n",
              "      <td>the day after tommorow</td>\n",
              "      <td>this week</td>\n",
              "      <td>next week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>today</td>\n",
              "      <td>next day</td>\n",
              "      <td></td>\n",
              "      <td>the weekend</td>\n",
              "      <td>week later</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tonight</td>\n",
              "      <td>following day</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>following week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this morning</td>\n",
              "      <td>one day</td>\n",
              "      <td></td>\n",
              "      <td>this weekend</td>\n",
              "      <td>next weekend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>current</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>week later</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>right now</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>coming week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>next weekend</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          today       tomorrow  ...      thisweek        nextWeek\n",
              "0           now       tomorrow  ...     this week       next week\n",
              "1         today       next day  ...   the weekend      week later\n",
              "2       tonight  following day  ...                following week\n",
              "3  this morning        one day  ...  this weekend    next weekend\n",
              "4       current                 ...                    week later\n",
              "5     right now                 ...                   coming week\n",
              "6                               ...                  next weekend\n",
              "\n",
              "[7 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN4pIshuLGOn"
      },
      "source": [
        "## Messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn1RVRSGykne",
        "outputId": "adc51309-3111-454e-c206-0c0f13d26fe0"
      },
      "source": [
        "def load_messages():\n",
        "  path_ = path\n",
        "  fname = 'messages.json'\n",
        "  messages = get_json(fname, False)\n",
        "  intents = {lang: messages[\"intents\"][\"language\"][lang] for lang in messages[\"intents\"][\"language\"]}\n",
        "  #MESSAGES_lang = [lang for lang in messages[\"messages\"][\"language\"]]\n",
        "  MESSAGES = {lang: messages[\"messages\"][\"language\"][lang] for lang in messages[\"messages\"][\"language\"]}\n",
        "  MESSAGES_lang = list(MESSAGES.keys())\n",
        "  MESSAGES_en, intents_en = [], []\n",
        "  if 'en' in MESSAGES:\n",
        "    MESSAGES_en = MESSAGES['en'][\"messages\"]\n",
        "    for part in intents['en'][\"messages\"]:\n",
        "      intents_en.append(part[\"responses\"])\n",
        "  # read also the new_mssages\n",
        "  \n",
        "  try:\n",
        "    with open(path + 'new_messages.json', \"r\") as f:\n",
        "      list_d = json.loads(f.read())\n",
        "  except Exception as error:\n",
        "    list_d = []\n",
        "  if len(list_d) > 0:\n",
        "    for dic in list_d:\n",
        "      intents.update(dic['intents']['language'])\n",
        "      MESSAGES.update(dic['messages']['language'])\n",
        "    MESSAGES_lang = list(MESSAGES.keys())\n",
        "  return MESSAGES_lang, MESSAGES, MESSAGES_en, intents, intents_en\n",
        "MESSAGES_lang, MESSAGES, MESSAGES_en, intents, intents_en = load_messages()\n",
        "p(MESSAGES_lang)\n",
        "p(intents_en)\n",
        "#intents['he'][\"messages\"][0]\n",
        "intents_en[1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "messages.json\n",
            "['en', 'he', 'fr', 'es', 'ar', 'ru']\n",
            "[[\"Hi. My name is WeatherBot and I'm a chatbot. If you want to exit, type 'Bye' If you want help type 'Help'\"], ['See you!', 'Have a nice day', 'Bye! Come back again soon.'], ['Happy to help!', 'Any time!', 'My pleasure'], [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand'], ['I can give you information about the weather in cities around the world, in your language. You can ask about clouds, rain, temp, pressure, humidity, wind, visibility, sunrise and upto 7 days from today'], ['WeatherBot', 'Ask Bot', 'Debug', 'User Input', 'You', 'Bot', 'ID', 'Conversation'], ['Done', 'sent']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['See you!', 'Have a nice day', 'Bye! Come back again soon.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCL5YLU-zuFS"
      },
      "source": [
        "def set_capitals():\n",
        "  # Getter that looks up the span text in the dictionary of country capitals\n",
        "  get_capital = lambda span: CAPITALS.get(span.text)\n",
        "\n",
        "  # Register the Span extension attribute \"capital\" with the getter get_capital\n",
        "  Span.set_extension(\"capital\", getter=get_capital, force=True)\n",
        "\n",
        "  # Process the text and print the entity text, label and capital attributes\n",
        "  doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
        "  print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS1pXmhO4ugH"
      },
      "source": [
        "# Call the web site"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhygq47xtL1r"
      },
      "source": [
        "{\"coord\":{\"lon\":34.8,\"lat\":32.0833},\n",
        "\n",
        "\"weather\":[{\"id\":801,\"main\":\"Clouds\",\"description\":\"few clouds\",\"icon\":\"02n\"}],\n",
        "\n",
        "\"base\":\"stations\",\n",
        "\n",
        "\"main\":{\"temp\":295.69,\"feels_like\":295.83,\"temp_min\":294.42,\"temp_max\":296.58,\"pressure\":1016,\"humidity\":70},\n",
        "\n",
        "\"visibility\":10000,\n",
        "\n",
        "\"wind\":{\"speed\":2.06,\"deg\":10},\n",
        "\n",
        "\"clouds\":{\"all\":20},\n",
        "\n",
        "\"dt\":1623961727,\n",
        "\n",
        "\"sys\":{\"type\":1,\"id\":6845,\"country\":\"IL\",\"sunrise\":1623897263,\"sunset\":1623948546},\n",
        "\n",
        "\"timezone\":10800,\n",
        "\n",
        "\"id\":293396,\n",
        "\n",
        "\"name\":\"Tel Aviv\",\n",
        "\n",
        "\"cod\":200}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkn5WmrShACd"
      },
      "source": [
        "def call_web(city_name, country_code, fahrenheit, one_call):\n",
        "    state = ''\n",
        "    api_url = f\"{URL}q={city_name},{state},{country_code}&units={'metric' if not fahrenheit else 'imperial'}&lang={LANG}&appid={api_key}\".format('weather')\n",
        "    #ic(city_name, country_code, fahrenheit, count)\n",
        "    if one_call:\n",
        "      cor = df_CITIES_API.loc[city_name].coord\n",
        "      if type(cor) is not dict: #more than one record for that city\n",
        "        for index, row in df_CITIES_API.loc[city_name].iterrows():\n",
        "          #ic('Z:', row['country'], row['coord'], country_code==row['country'])\n",
        "          if row['coord'] != '' and (country_code==row['country'] or country_code == ''):\n",
        "            cor = row['coord']\n",
        "            break\n",
        "      api_url = f\"{URL}lat={cor['lat']}&lon={cor['lon']}&exclude=minutely,hourly&units={'metric' if not fahrenheit else 'imperial'}&lang={LANG}&appid={api_key}\".format('onecall')\n",
        "\n",
        "    ic(api_url)\n",
        "\n",
        "    response = requests.get(api_url)\n",
        "    response_dict = response.json()\n",
        "\n",
        "    weather = response_dict #[\"weather\"]#[0]#[\"description\"]\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return weather\n",
        "    elif response.status_code == 404:\n",
        "        return None\n",
        "    else:\n",
        "        print(MESSAGES[LANG][\"messages\"][\"web_error\"].format(response.status_code, api_url))\n",
        "        ic(response_dict['message'])\n",
        "        return None"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88obffuE3wyl"
      },
      "source": [
        "def get_weather_info(actions, one_call, cnt, city_weather, default, LANG, DISP):\n",
        "  all_action = actions[0][0] == 'all'  # always first\n",
        "  #one_call, cnt, city_weather)\n",
        "  result = ''\n",
        "  for action in actions:\n",
        "    tag = action[0]\n",
        "    if tag == \"fahrenheit\": continue\n",
        "    user_act = action[1]\n",
        "    if not one_call:\n",
        "      data = city_weather if tag == \"all\" else city_weather['main'] \\\n",
        "        if tag in [\"weatherTemp\", \"weatherPressure\", 'temp', \"pressure\", \"humidity\"] else \\\n",
        "              city_weather['sys'] if tag == \"sunrise\" else city_weather['wind'] \\\n",
        "              if tag in [\"weatherWind\", \"wind\"] else city_weather[tag] \n",
        "    else:\n",
        "      data = city_weather\n",
        "    if DISP: ic('in loop', user_act, tag, data)\n",
        "    if tag in ['clouds']:\n",
        "      clouds_percentage = data['all'] if not one_call else data['clouds']\n",
        "      if clouds_percentage < 10: # no clouds\n",
        "        if 'clear sky' not in default.lower():\n",
        "          result += f'{MESSAGES[LANG][\"messages\"][\"clear sky\"]}, '\n",
        "        elif not all_action:\n",
        "          result += f' {default},'\n",
        "      elif default not in result:\n",
        "        result += f' {default},'\n",
        "      if all_action: continue\n",
        "    if user_act == \"rain\": \n",
        "      if user_act in default.lower():\n",
        "        if default not in result:\n",
        "          result += f' {default},'\n",
        "      else:\n",
        "        result += f' {MESSAGES[LANG][\"messages\"][\"no\"]} {user_act},'\n",
        "      if all_action: continue\n",
        "    if all_action and tag != 'all': continue\n",
        "    if tag in [\"weather\", \"weatherPressure\",\"weatherWind\",\"weatherTemp\",\"all\"]:\n",
        "      if default not in result:\n",
        "        result += f' {default},'\n",
        "    if tag in [\"weatherTemp\", \"weatherPressure\", 'temp', \"pressure\", \"humidity\", \"all\"]:\n",
        "      data_tag = data['main'] if tag == 'all' and not one_call else data\n",
        "      #one_call, tag, data_tag, data)\n",
        "      if tag in [ \"weatherTemp\", \"temp\", \"all\"]:\n",
        "        for i, temp in enumerate(MESSAGES[LANG][\"messages\"][\"temp\"]):\n",
        "          if i == 0: \n",
        "            if cnt == 0:\n",
        "              info = data_tag[\"temp\"]\n",
        "            else:\n",
        "              info = data_tag[\"temp\"]['day']\n",
        "          elif i == 1: \n",
        "            if cnt == 0:\n",
        "              info = data_tag[\"feels_like\"]\n",
        "            else:\n",
        "              info = data_tag[\"feels_like\"]['day']\n",
        "            if one_call  and cnt == 0:\n",
        "              break\n",
        "          elif i == 2: info = data_tag[\"temp_min\"] if cnt == 0 else data_tag[\"temp\"][\"min\"]\n",
        "          elif i == 3: info = data_tag[\"temp_max\"] if cnt == 0 else data_tag[\"temp\"][\"max\"]\n",
        "          result += f' {temp} = {info},'\n",
        "      if tag in [\"weatherPressure\", \"pressure\", \"all\"]:\n",
        "        result += f' {MESSAGES[LANG][\"messages\"][\"pressure\"]} = {data_tag[\"pressure\"]},'\n",
        "      if tag in [\"humidity\", \"all\"]:\n",
        "        result += f' {MESSAGES[LANG][\"messages\"][\"humidity\"]} = {data_tag[\"humidity\"]},'\n",
        "    if tag in [\"wind\", \"weatherWind\", \"all\"]:\n",
        "      data_tag = data['wind'] if tag == 'all' and not one_call else data\n",
        "      for i, wind in enumerate(MESSAGES[LANG][\"messages\"][\"wind\"]):\n",
        "        if i == 0: info = data_tag[\"speed\"] if not one_call else data_tag[\"wind_speed\"]\n",
        "        elif i == 1: info = data_tag[\"deg\"] if not one_call else data_tag[\"wind_deg\"]\n",
        "        elif i == 2: \n",
        "          if \"gust\" in data_tag: \n",
        "            info = data_tag[\"gust\"] \n",
        "        elif \"wind_gust\" in data_tag: \n",
        "            info = data_tag[\"wind_gust\"] \n",
        "        else: \n",
        "            continue\n",
        "        result += f' {wind} = {info},'\n",
        "    if tag in [\"visibility\",\"all\"]:\n",
        "      if cnt > 0 and 'visibility' not in data:\n",
        "        data_tag = MESSAGES[LANG][\"messages\"][\"no_info\"]\n",
        "      else:\n",
        "        data_tag = data['visibility'] if tag == 'all' or one_call else data\n",
        "      result += f' {MESSAGES[LANG][\"messages\"][\"visibility\"]} = {data_tag},'\n",
        "    if tag in [\"sunrise\",\"all\"]:\n",
        "      data_tag = data['sys'] if tag == 'all' and not one_call else data\n",
        "      for i, wind in enumerate(MESSAGES[LANG][\"messages\"][\"sunrise\"]):\n",
        "        if i == 0: info = data_tag[\"sunrise\"]\n",
        "        elif i == 1: info = data_tag[\"sunset\"]\n",
        "        result += f' {wind} = {time.strftime(\"%H:%M:%S\", time.gmtime(info + 3 * 60 * 60))},'\n",
        "  return result"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1XIdfNeh1d5"
      },
      "source": [
        "def get_weather(days, location, actions, range_days, LANG=LANG, DISP=False):\n",
        "  #dt, location)\n",
        "  city_w = city = location[0][0]  # assume one city (not handling if there are 2 cities)\n",
        "  country = location[0][1]\n",
        "  #city, city_w)\n",
        "  result = ''\n",
        "\n",
        "  #convert israeli cities to district that the API knows ('sharon' is in API because there is such city in Australia)\n",
        "  if country == 'israel' and city in cities_il_df.index.values:\n",
        "    city_w = cities_il_df.loc[city].district\n",
        "\n",
        "  #city_w, location)\n",
        "  #country, len(countries_df.Name))\n",
        "  if country in list(countries_df.Name):\n",
        "    country_code = countries_df[countries_df.Name == country].index[0]\n",
        "  else:\n",
        "    country_code = ''\n",
        "  #f'Y: {city_w}, [{country_code}]')\n",
        "\n",
        "  fahrenheit = True if len([action for action in actions if action[0] == \"fahrenheit\"]) > 0 else False\n",
        "  #p('in get_weather:', fahrenheit)\n",
        "  #p(city_w, country_code, fahrenheit, one_call)\n",
        "  one_call = (days > 0 or range_days > 1)\n",
        "  p(one_call)\n",
        "  city_weather_resp = call_web(city_w, country_code, fahrenheit, one_call)\n",
        "\n",
        "  if DISP: ic(city_weather_resp)\n",
        "  if DISP: ic('get_w', actions)\n",
        "  if city_weather_resp is None:\n",
        "    return MESSAGES[LANG][\"messages\"][\"api_error\"], ''\n",
        "    # check what informtion the user asked for\n",
        "\n",
        "  city_weather = city_weather_resp if not one_call else city_weather_resp[\"current\"]\n",
        "\n",
        "  default = city_weather[\"weather\"][0][\"description\"]\n",
        "  icon = city_weather[\"weather\"][0][\"icon\"]\n",
        "  url_icon = f\"{icon_url}\".format(icon)\n",
        "  if DISP: ic(url_icon)\n",
        "  if DISP: ic('get_w', default)\n",
        "  result = ''\n",
        "\n",
        "  for cnt in range(days, 9):\n",
        "    if cnt > 0: \n",
        "      # bug in python - cannot have r += z + x if cond else y\n",
        "      M = MESSAGES[LANG][\"messages\"][\"web_days_msg\"].format(cnt) if cnt > 1 else MESSAGES[LANG][\"messages\"][\"web_day_msg\"]\n",
        "      result += '<br>' + M\n",
        "      city_weather = city_weather_resp[\"daily\"][0]\n",
        "    new_result = get_weather_info(actions, one_call, cnt, city_weather, default, LANG, DISP)\n",
        "    #cnt, new_result)\n",
        "    result += new_result\n",
        "    if not one_call: break\n",
        "\n",
        "  if result == '': result = default\n",
        "  if result[-1] == ',': result = result[:-1]\n",
        "  country = location[0][1]\n",
        "  if LANG != 'en': # in case there are parts of the answer which are not in the proper langauge\n",
        "    response = translator.translate(city, dest=LANG) # need to separet ity from country in case one of them has multi words and google doe not keep the qoutation mark as needed\n",
        "    #response = translator.translate(f\"({city}, {country})\", dest=LANG)\n",
        "    #city, country = response.text\n",
        "    p(f\"{response.origin} ({response.src}) --> {response.text} ({response.dest})\")\n",
        "    city = response.text\n",
        "    response = translator.translate(country, dest=LANG)\n",
        "    country = response.text\n",
        "  prompt = MESSAGES[LANG][\"messages\"][\"web_msg\"] if days == 0 else MESSAGES[LANG][\"messages\"][\"web_msg_no_current\"]\n",
        "  response =  prompt.format(city.capitalize(), country.capitalize(), result)\n",
        "  return response, url_icon\n",
        "r = '''    \n",
        "#answer = get_weather('current', [('jerusalem', 'israel')], [('clouds', 'cloud'), ('temp', 'temp'), ('pressure', 'pressure'), ('humidity', 'humidity'), ('wind', 'wind'), ('visibility', 'wind'), ('sunrise', 'wind'), ('weather','rain')])\n",
        "answer = get_weather('current', [('jerusalem', 'israel')], \n",
        "  [('temp', 'heat'),\n",
        "  ('weather', 'rain'),\n",
        "  ('temp', 'temp'),\n",
        "  ('pressure', 'pressure'),\n",
        "  ('humidity', 'humidity'),\n",
        "  ('wind', 'wind'),\n",
        "  ('visibility', 'visibility'),\n",
        "  ('sunrise', 'sunrise')])\n",
        "'''\n",
        "#{'date': 'current', 'action': ('weather', 'weather'), 'city': [('vi', 'sweden')]}\n",
        "# if tag in [\"weatherTemp\", \"weatherPressure\", 'temp', \"pressure\", \"humidity\", \"all\"]:\n",
        "\n",
        "#answer = get_weather('current', [('toronto', 'canada')],  [('all', 'all'), ('visibility', 'visibility'), ('sunrise', 'sunrise'), ('temp', 'heat'), ('wind', 'wind'), ('weather','rain'), ('fahrenheit','fahrenheit')], DISP=False)\n",
        "#answer"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn0OMy29XzlX"
      },
      "source": [
        "# NLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-yUaWlmydDs"
      },
      "source": [
        "FAST_START = False"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "n6sXcxyxQTlv",
        "outputId": "7e37e4e0-5c80-4df1-9626-5f2ccec0ee7a"
      },
      "source": [
        "get_parts(\"weather currently in tel aviv\", True)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokens of [weather]: dep_=ROOT, ent_type=, head=weather, lemma_=weather, pos_=NOUN, tag_=NN, is_action=True\n",
            "Tokens of [currently]: dep_=advmod, ent_type=DATE, head=in, lemma_=currently, pos_=ADV, tag_=RB, is_action=False\n",
            "Tokens of [in]: dep_=prep, ent_type=, head=weather, lemma_=in, pos_=ADP, tag_=IN, is_action=False\n",
            "Tokens of [tel]: dep_=compound, ent_type=GPE, head=aviv, lemma_=tel, pos_=PROPN, tag_=NNP, is_action=False\n",
            "Tokens of [aviv]: dep_=pobj, ent_type=GPE, head=in, lemma_=aviv, pos_=PROPN, tag_=NNP, is_action=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk\\nimport bokeh\\nimport fbprophet\\nimport pydot\\nfrom sklearn.model_selection import cross_val_score\\nimport pandas as pd\\nfrom xlrd import open_workbook\\nimport os\\nimport plotly as py\\nimport pickle\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport numpy as np\\nimport altair as alt\\nfrom sklearn.manifold import TSNE\\nimport imutils\\nimport spacy\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport re\\nimport fastai\\nfrom statsmodels.tsa.arima_model import ARIMA\\nimport plotly.express as px\\nimport matplotlib.pyplot as plt\\nimport cv2'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [currently]: DATE='Absolute or relative dates or periods', is_city=True, is_country=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk\\nimport bokeh\\nimport fbprophet\\nimport pydot\\nfrom sklearn.model_selection import cross_val_score\\nimport pandas as pd\\nfrom xlrd import open_workbook\\nimport os\\nimport plotly as py\\nimport pickle\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport numpy as np\\nimport altair as alt\\nfrom sklearn.manifold import TSNE\\nimport imutils\\nimport spacy\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport re\\nimport fastai\\nfrom statsmodels.tsa.arima_model import ARIMA\\nimport plotly.express as px\\nimport matplotlib.pyplot as plt\\nimport cv2'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [tel aviv]: GPE='Countries, cities, states', is_city=False, is_country=False\n",
            "{'date': ['currently'], 'time': [], 'action': [('all', 'weather')], 'state': ['tel aviv']}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action': [('all', 'weather')],\n",
              " 'date': ['currently'],\n",
              " 'state': ['tel aviv'],\n",
              " 'time': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Pb9JLByJsG9R",
        "outputId": "359f3b8c-2b1e-4841-b5fd-a834a85b52e7"
      },
      "source": [
        "nlp = en_core_web_md.load()\n",
        "'''\n",
        " becuase it takes long time to init (about 1.5 minutes), and once a day the PythonAnywhere bring down all sites,\n",
        " for maintenance, and to prevet the first user from waiting too long, I adde a line in the nightly batch file\n",
        " rungui.bat that start the site atabout 3 AM\n",
        "  start chrome http://drbarak.pythonanywhere.com/chatbot\n",
        "'''\n",
        "def update_nlp():\n",
        "  matcher = PhraseMatcher(nlp.vocab)\n",
        "  date_time = ['now', 'current', 'currently']\n",
        "  date_time_str = \"now, current, currently\"\n",
        "  CITIES = set(CITIES_IL + CITIES_API_city + date_time)# + ACTIONS_patterns) it recognized all the actions as GPE\n",
        "  #CITIES = list(CITIES)[:10]\n",
        "  #CITIES)\n",
        "  # disable other pipes while doing the traing to speed it up - went down from 1:40 to 0:40 minutes\n",
        "  if not FAST_START:\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):\n",
        "      matcher.add(\"CITIES\", None, *list(nlp.pipe(CITIES)))\n",
        "#      matcher.add(\"ACTIONS\", None, *list(nlp.pipe(ACTIONS_patterns)))\n",
        "\n",
        "  # country codes causes problems because many are similar to regular words (eg. 'IN' similar to 'in')\n",
        "  #matcher.add(\"CITIES_API_country_code\", None, *list(nlp.pipe(CITIES_API_country_code)))\n",
        "  # due to duplicates with country codes ('IL' = Israel, and Ilinoie), we have to check manually for states\n",
        "  #matcher.add(\"CITIES_API_state_code\", None, *list(nlp.pipe(CITIES_API_state_code)))\n",
        "\n",
        "  # Register the Span extension attributes \"is_country\" and \"is_city\" \n",
        "  check_country = lambda span: span.text.lower().translate(remove_punct_dict) in COUNTRIES\n",
        "  Span.set_extension('is_country', getter=check_country, force=True)\n",
        "\n",
        "  check_city = lambda span: span.text.lower().translate(remove_punct_dict) in CITIES\n",
        "  Span.set_extension('is_city', getter=check_city, force=True)\n",
        "\n",
        "  check_action = lambda token: token.text.lower().translate(remove_punct_dict) in ACTIONS_patterns or \\\n",
        "                                token.lemma_.lower().translate(remove_punct_dict) in ACTIONS_patterns\n",
        "  Token.set_extension('is_action', getter=check_action, force=True)\n",
        "\n",
        "  pipes = nlp.pipe_names\n",
        "  if 'gpe_component' in pipes:\n",
        "    nlp.remove_pipe('gpe_component')\n",
        "\n",
        "  if spacy.__version__ > '3':\n",
        "    @Language.component(\"gpe_component\")\n",
        "    def gpe_component(doc):\n",
        "        # Create an entity Span with the label \"GPE\" for all matches\n",
        "        matches = matcher(doc)\n",
        "        try:\n",
        "          doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
        "          #if len(doc.ents) > 0:\n",
        "          #  doc.ents, 'rain' in CITIES)\n",
        "          #doc.ents += tuple([Span(doc, start, end, label=\"WACT\") for match_id, start, end in matches if nlp.vocab.strings[match_id] == 'ACTIONS'])\n",
        "        except ValueError as e: #[E103] Trying to set conflicting doc.ents: '(2, 4, 'GPE')' and '(3, 4, 'GPE')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.\n",
        "          if 'E103' in e.args[0]:\n",
        "            return doc  # use the standard model of Spacy, hopefully it works (as in the case \"weather in new york\") or  other rounds will get the proper result\n",
        "          raise\n",
        "        return doc\n",
        "\n",
        "    # Add the new component to the pipeline\n",
        "    nlp.add_pipe('gpe_component', first=True)\n",
        "  else:\n",
        "    def gpe_component(doc):\n",
        "        # Create an entity Span with the label \"GPE\" for all matches\n",
        "        matches = matcher(doc)\n",
        "        try:\n",
        "          #doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches if nlp.vocab.strings[match_id] == 'CITIES']\n",
        "          doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches if doc[start:end].text not in date_time_str]\n",
        "          doc.ents += tuple([Span(doc, start, end, label=\"DATE\") for match_id, start, end in matches if doc[start:end].text in date_time_str])\n",
        "          '''\n",
        "          doc_ents = []\n",
        "          for match_id, start, end in matches:\n",
        "            ent = doc[start:end].text\n",
        "            p(ent, start, end, ent in date_time_str)\n",
        "            if ent in date_time_str:\n",
        "              ent = Span(doc, start, end, label=\"DATE\")\n",
        "            else:\n",
        "              ent = Span(doc, start, end, label=\"GPE\")\n",
        "            doc_ents.append(ent)\n",
        "          doc.ents = doc_ents\n",
        "          '''\n",
        "#          if len(doc.ents) > 0:\n",
        "#            doc.ents, 'rain' in CITIES)\n",
        "\n",
        "#          doc.ents += tuple([Span(doc, start, end, label=\"WACT\") for match_id, start, end in matches if nlp.vocab.strings[match_id] == 'ACTIONS'])\n",
        "        except ValueError as e: #[E103] Trying to set conflicting doc.ents: '(2, 4, 'GPE')' and '(3, 4, 'GPE')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.\n",
        "          if 'E103' in e.args[0]:\n",
        "            return doc  # use the standard model of Spacy, hopefully it works (as in the case \"weather in new york\") or  other rounds will get the proper result\n",
        "          raise\n",
        "        return doc\n",
        "\n",
        "    # Add the new component to the pipeline\n",
        "    nlp.add_pipe(gpe_component, first=True)\n",
        "  #print(nlp.pipe_names)\n",
        "\n",
        "update_nlp()\n",
        "#get_parts(\"humidity nenita ellerbach\", True)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk\\nimport bokeh\\nimport fbprophet\\nimport pydot\\nfrom sklearn.model_selection import cross_val_score\\nimport pandas as pd\\nfrom xlrd import open_workbook\\nimport os\\nimport plotly as py\\nimport pickle\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport numpy as np\\nimport altair as alt\\nfrom sklearn.manifold import TSNE\\nimport imutils\\nimport spacy\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport re\\nimport fastai\\nfrom statsmodels.tsa.arima_model import ARIMA\\nimport plotly.express as px\\nimport matplotlib.pyplot as plt\\nimport cv2'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpjr3Yesh6-f"
      },
      "source": [
        "# Get Parts (POS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLZTnpUyhjHl"
      },
      "source": [
        "## check date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaXFn9Q-6HLR",
        "outputId": "b2f368b3-de58-4ced-c1dd-3a9f477dde61"
      },
      "source": [
        "def remove_stop_words(doc1_list):\n",
        "  user_list = []\n",
        "  for doc1 in doc1_list:\n",
        "    user_msg, space  = '', ''\n",
        "    for token in doc1.split():\n",
        "      if token in stopwords: continue\n",
        "      text = token.lower().translate(remove_punct_dict)\n",
        "      if text == '': continue\n",
        "      user_msg += space + text\n",
        "      space = ' '\n",
        "    user_list.append(user_msg)\n",
        "  return user_list\n",
        "p(remove_stop_words(['friday']))\n",
        "p(remove_stop_words(['friday','in 2 weeks']))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['friday']\n",
            "['friday', '2 weeks']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcEaNlVju4YB"
      },
      "source": [
        "TODAY = 0 # so can test different days"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l5DzMPtPQPW"
      },
      "source": [
        "NO_DAYS = -999\n",
        "def get_delta_days_weekday(text, dayInText_org=-NO_DAYS, weekend = False, range_days = 0, disp=False):\n",
        "  if disp: p('get_delta_days_weekday', dayInText_org)\n",
        "  days, week, dayInText = NO_DAYS, -1, dayInText_org\n",
        "  if TODAY == 0:\n",
        "    todayDate = datetime.now(tz)\n",
        "    #todayDayWeek=(todayDate.weekday()+2-7) if (todayDate.weekday()+2>7) else  (todayDate.weekday()+2) # get day of week today\n",
        "    #todayDayWeek = (todayDate.weekday() + 1) % 7 + 1 # get day of the week such that sunday=1,.. saturday=7\n",
        "    todayDayWeek = todayDate.weekday() + 2 if todayDate.weekday() > 1 else 7 # from day of the week such that sunday=1,.. saturday=7\n",
        "  else: \n",
        "    todayDayWeek = TODAY\n",
        "  if disp: p(todayDayWeek)\n",
        "\n",
        "  if text in list(dayInWeek.keys()): # if there is only one word of day in week\n",
        "    if dayInText_org == -NO_DAYS: # if we did not find a day already\n",
        "      dayInText = dayInWeek[text]\n",
        "      if disp: p(text, dayInText, dayInWeek)\n",
        "      if todayDayWeek <= dayInText:\n",
        "        days = dayInText - todayDayWeek\n",
        "      else:\n",
        "        days = dayInText - todayDayWeek + 7\n",
        "  else:# if there are additional word like 'next sunday'\n",
        "    doc = nlp(text)\n",
        "    for entity in doc:\n",
        "      if disp: print(f'entity = {entity.text}, lemma_ = {entity.lemma_}')\n",
        "      if entity.text in list(dayInWeek.keys()):\n",
        "        dayInText = dayInWeek[entity.text.lower()]\n",
        "        if todayDayWeek <= dayInText:\n",
        "          days = dayInText - todayDayWeek\n",
        "        else:\n",
        "          days = dayInText - todayDayWeek + 7\n",
        "          week = 0\n",
        "      elif entity.lemma_ == 'next' and week == -1:\n",
        "        if disp: print('get next')\n",
        "        week = 1\n",
        "      elif entity.lemma_ == 'weekend' and not weekend and week == -1:\n",
        "        week = 1\n",
        "        range_days = 2\n",
        "        weekend = True\n",
        "  if week == -1:\n",
        "    week = 0\n",
        "  if disp: p('get_delta_days_weekday: dayInText =', dayInText)\n",
        "  return days, week, todayDayWeek, dayInText, weekend, range_days          \n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDvMjuj9PTvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3df34e7-a29f-4c9b-f150-28f812c4d730"
      },
      "source": [
        "p(get_delta_days_weekday(\"friday\"))\n",
        "get_delta_days_weekday(\"sunday\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 0, 7, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 0, 7, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhJd9-jkPXFe"
      },
      "source": [
        "def get_number_period(doc, disp=False):\n",
        "  if disp: p('get_number_period')\n",
        "  days, week = NO_DAYS, 0\n",
        "  for entity in doc:\n",
        "    if disp: print(entity.pos_, entity.head, entity.lemma_)\n",
        "    if entity.pos_=='NUM':\n",
        "      if disp: print(\"found number \"+entity.text)\n",
        "      if entity.text.isdigit():\n",
        "          num = int(entity.text)\n",
        "      else:\n",
        "          num = textToNumbers.get(entity.text, NO_DAYS)\n",
        "      if disp: print ('number is '+str(num))\n",
        "      if nlp(str(entity.head))[0].lemma_ == 'week': # if there is week add it to number to week and if not match it num of days\n",
        "        if num != NO_DAYS:\n",
        "          week = num\n",
        "      else:\n",
        "        days = num\n",
        "  #if week > 0 and days == NO_DAYS:  days = 0\n",
        "  return days, week        "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_pAjYkTa-TP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee060c2-0b44-4342-891c-c2ae240afa44"
      },
      "source": [
        "def check_date(dateText, disp=False):\n",
        "  days, week, found_days, cur_days = NO_DAYS, 0, NO_DAYS, NO_DAYS\n",
        "  todayDayWeek = 0 # get day in week today\n",
        "  dayInText = -NO_DAYS # get day in week by text\n",
        "  f_string = \" days={0}, week={1}, found_days={2}, cur_days={3}\"\n",
        "  range_days, weekend = 1, False # single day\n",
        "\n",
        "  if len(dateText) == 0:\n",
        "    return NO_DAYS, 0\n",
        "  for text in dateText: # check if we get several date variables\n",
        "    if disp: p('text =', text)\n",
        "    #matcher = PhraseMatcher(nlp.vocab)\n",
        "    for column in datesDf: # run on each column if there is match\n",
        "      #listNlp = [nlp.make_doc(t) for t in df[column].tolist() if str(t) != 'nan']\n",
        "      date_matcher = PhraseMatcher(nlp.vocab)\n",
        "      date_matcher.add(column, None, *listNlp[column])\n",
        "      #p(listNlp[column])\n",
        "      doc = nlp(text.lower())\n",
        "      matches = date_matcher(doc)\n",
        "      if disp: p(column, matches)\n",
        "      if len(matches) > 0: # if we find match in the list of column\n",
        "        match_words = ''\n",
        "        for match_id, start, end in matches:\n",
        "          match_words += f\"{doc[start:end].text}, \"\n",
        "        if disp:\n",
        "          p('col:',column)\n",
        "          print(\"matchFound\", match_words)\n",
        "        if column == \"nextWeek\": # check if there \n",
        "          if disp: print(\"is next Week\")\n",
        "          week = 1\n",
        "          if days == NO_DAYS and found_days == NO_DAYS: days = 0\n",
        "          if 'weekend' in match_words:\n",
        "            range_days = 2\n",
        "            weekend = True\n",
        "        elif 'weekend' in match_words:\n",
        "          range_days = 2\n",
        "          weekend = True\n",
        "        if len(dateText) == 1:        \n",
        "          numFinal = dateToNum.get(column, NO_DAYS)\n",
        "          if numFinal != NO_DAYS or days == NO_DAYS:\n",
        "            return numFinal, range_days\n",
        "        if disp: p(days, todayDayWeek, dayInText)\n",
        "        # todayDayWeek is in the range 1=sunday to 7=saturday\n",
        "        # dayInText is in the same range\n",
        "        # in order to compare we need to convert both such the sunday = 8 (mon=1, tue=2...sat=7, sun=8)\n",
        "        todayDayWeek_new = todayDayWeek if todayDayWeek > 1 else 8\n",
        "        dayInText_new = dayInText if dayInText > 1 else 8\n",
        "        if disp: p(days, todayDayWeek, dayInText, dayInText_new)\n",
        "        if days != NO_DAYS:  # verify days match numOfdays (in case on Friday asked for \"friday in 2 days\" which is wrong)\n",
        "          if column == \"today\" and days == 0:\n",
        "            pass\n",
        "          elif column == \"tomorrow\" and days == 1:\n",
        "            pass\n",
        "          elif column == \"twoDays\" and days == 2:\n",
        "            pass\n",
        "          elif column == 'thisweek' and (dayInText_new >= todayDayWeek_new):  # this week must be day after today up to sunday\n",
        "            pass\n",
        "          else:\n",
        "            days = NO_DAYS\n",
        "        cur_days = days  # days for this text independent of prev columns\n",
        "        break # found a natch no need to check others\n",
        "    if disp: print(\"1.\" + f_string.format(days, week, found_days, cur_days))\n",
        "    if found_days == NO_DAYS: found_days = days\n",
        "    if week == 0 and cur_days == NO_DAYS: # didn't find word in the DF dates\n",
        "\n",
        "      days, week, todayDayWeek, dayInText, weekend, range_days = get_delta_days_weekday(text.lower(), dayInText, weekend, range_days, disp) # check day in week and return num of days and week\n",
        "\n",
        "      if disp: print(\"2.\" + f_string.format(days, week, found_days, cur_days))\n",
        "      if days != NO_DAYS and days != found_days and found_days != NO_DAYS:\n",
        "        days = NO_DAYS\n",
        "      elif found_days == NO_DAYS:\n",
        "        found_days = days\n",
        "      if week == 0 or days == NO_DAYS:\n",
        "        sav_days = days\n",
        "\n",
        "        days, week = get_number_period(doc, disp) # check number of days/weeks\n",
        "        if days == NO_DAYS and sav_days != NO_DAYS:\n",
        "          days = sav_days\n",
        "        if disp: print(\"3.\" + f_string.format(days, week, found_days, cur_days))\n",
        "        # in english the week ends on Sunday, so if we r on:\n",
        "        # sunday then next_week=0 weeks from now\n",
        "        # monday then next_week=1 weeks from now\n",
        "        # -> we calc days up to sunday, and add 1 week\n",
        "\n",
        "  if disp: print(\"4.\" + f_string.format(days, week, found_days, cur_days))\n",
        "  if disp: p('todayDayWeek', todayDayWeek, dayInText, days, weekend)\n",
        "  org_days = days\n",
        "  if todayDayWeek == 0 or (days == found_days == cur_days == -dayInText == NO_DAYS):  # no day information -> today\n",
        "    if TODAY == 0:\n",
        "      dt = datetime.now(tz)\n",
        "      days = dt.weekday() + 2 if dt.weekday() > 1 else 7 # from day of the week such that sunday=1,.. saturday=7\n",
        "    else:\n",
        "      days = TODAY\n",
        "    todayDayWeek = dayInText = days\n",
        "    found_days = days = 0 # to convert to #days from saturday (0 if saturday, 1=sunday etc)\n",
        "    if disp: p(todayDayWeek, dayInText, days, found_days)\n",
        "  if days != NO_DAYS and days != found_days and found_days != NO_DAYS:\n",
        "    days = NO_DAYS\n",
        "  elif days != NO_DAYS and found_days == NO_DAYS:\n",
        "    found_days = days\n",
        "  elif days == NO_DAYS and week > 0:\n",
        "    days = found_days\n",
        "    if days == NO_DAYS:\n",
        "      days = 0 # no day is specified so days = 0 (number of days since today)\n",
        "  if days != NO_DAYS:\n",
        "    if disp: p(todayDayWeek, dayInText)\n",
        "    # convert to english weekday number (monday=1... sunday=7)\n",
        "    todayDayWeek = todayDayWeek - 1 if todayDayWeek > 1 else 7 # from our day sunday=1,.. saturday=7\n",
        "    dayInText =  dayInText - 1 if dayInText > 1 else 7\n",
        "    if disp: p(todayDayWeek, dayInText, week, days)    \n",
        "    if todayDayWeek > dayInText and week > 0:\n",
        "      week -= 1\n",
        "    if range_days == 2 and weekend:\n",
        "      if disp: p('weekend', org_days, org_days)\n",
        "      if org_days == NO_DAYS or dayInText >= 6:\n",
        "        if dayInText == 7: # if sunday there is one day left in the weekend\n",
        "          if week == 1: # next week\n",
        "            days -= 1 # nextweekent starts on Saturday\n",
        "          else:\n",
        "            range_days = 1\n",
        "        elif dayInText < 6: # not sunday and not staurday\n",
        "          days = 7 - dayInText # number of days to saturday\n",
        "      else:\n",
        "        days = NO_DAYS  # can not say 'monday, next weekend\n",
        "    return week * 7 + days, range_days\n",
        "  return NO_DAYS, 0\n",
        "#check_date([\"sunday\",\"next month\"], True)\n",
        "#p('A:', check_date([\"this weekend\"], False))\n",
        "#p('A:', check_date([\"weekend\"], False))\n",
        "#p('A:', check_date([\"next weekend\"], False))\n",
        "#p('A:', check_date([\"current\"], False))\n",
        "#p('A:', check_date([\"currently\"], True))\n",
        "#p('A:', check_date([\"test\"], True))\n",
        "check_date(get_parts(\"weather tomorrow in tel aviv\", False)['date'], True)\n"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text = tomorrow\n",
            "today []\n",
            "tomorrow [(3573583789758258062, 0, 1)]\n",
            "col: tomorrow\n",
            "matchFound tomorrow, \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc52p5WOzdQJ"
      },
      "source": [
        "TODAY = 0 # so can test different days"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-j3NCFjyGgJ",
        "outputId": "ba4df821-5566-4905-cd01-08e74cde9f6b"
      },
      "source": [
        "if TODAY == 0:\n",
        "  dt = datetime.now(tz)\n",
        "  dayofweek = list(dayInWeek.keys())[list(dayInWeek.values()).index((dt.weekday() + 1) % 7 + 1)]\n",
        "  ndays = (dt.weekday() + 1) % 7 + 1\n",
        "else:\n",
        "  dayofweek = list(dayInWeek.keys())[list(dayInWeek.values()).index(TODAY)]\n",
        "  ndays = (TODAY + 1) % 7 + 1\n",
        "\n",
        "p(f'today is {dayofweek}')#, checking for {day}, {ndays}')\n",
        "\n",
        "for day in dayInWeek.keys():\n",
        "  print(day, '   \\tA:', check_date([day,\"next week\"], False), end=', ')\n",
        "  print('B:', check_date([day,\"in one week\"], False), end=', ')\n",
        "  print('C:', check_date([day,\"2 days\"], False), end=', ')\n",
        "  print('D:', check_date([day,\"one week\"], False), end=', ')\n",
        "  print('E:', check_date([day,\"1 week\"], False), end=', ')\n",
        "  print('F:', check_date([day,\"one day\"], False), end=', ')\n",
        "  print('G:', check_date([day], False), end=', ')\n",
        "  print('H:', check_date([day,\"following week\"], False), end=', ')\n",
        "  print('I:', check_date([day,\"week later\"], False), end=', ')\n",
        "  print('J:', check_date([day,\"coming week\"], False), end=', ')\n",
        "  print('K:', check_date([day,\"this week\"], False), end=', ')\n",
        "  print('L:', check_date([day,\"this weekend\"], False), end=', ')\n",
        "  print('M:', check_date([day,\"next weekend\"], False), end=', ')\n",
        "  print('N:', check_date([day,\"weekend\"], False), end=', ')\n",
        "  \n",
        "  if day == dayofweek:\n",
        "    p()\n",
        "    print('\\ta:', check_date([\"next week\"], False), end=', ')\n",
        "    print('b:', check_date([\"in one week\"], False), end=', ')\n",
        "    print('c:', check_date([\"2 days\"], False), end=', ')\n",
        "    print('d:', check_date([\"one week\"], False), end=', ')\n",
        "    print('e:', check_date([\"1 week\"], False), end=', ')\n",
        "    print('f:', check_date([\"one day\"], False), end=', ')\n",
        "    print('g:', check_date([\"following week\"], False), end=', ')\n",
        "    print('h:', check_date([\"week later\"], False), end=', ')\n",
        "    print('i:', check_date([\"coming week\"], False), end=', ')\n",
        "    print('j:', check_date([\"this week\"], False), end=', ')\n",
        "    print('k:', check_date([day,\"this weekend\"], False), end=', ')\n",
        "    print('l:', check_date([day,\"next weekend\"], False), end=', ')\n",
        "    print('m:', check_date([day,\"weekend\"], False), end=', ')\n",
        "  p()\n",
        "p()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "today is monday\n",
            "sunday    \tA: (13, 1), B: (13, 1), C: (-999, 0), D: (13, 1), E: (13, 1), F: (-999, 0), G: (6, 1), H: (13, 1), I: (13, 1), J: (13, 1), K: (6, 1), L: (6, 1), M: (12, 2), N: (-999, 0), \n",
            "monday    \tA: (7, 1), B: (7, 1), C: (-999, 0), D: (7, 1), E: (7, 1), F: (-999, 0), G: (0, 1), H: (7, 1), I: (7, 1), J: (7, 1), K: (0, 1), L: (-999, 2), M: (13, 2), N: (-999, 0), \n",
            "\ta: (7, 1), b: (7, 1), c: (2, 1), d: (7, 1), e: (7, 1), f: (1, 1), g: (7, 1), h: (7, 1), i: (7, 1), j: (0, 1), k: (-999, 2), l: (13, 2), m: (-999, 0), \n",
            "tuesday    \tA: (8, 1), B: (8, 1), C: (-999, 0), D: (8, 1), E: (8, 1), F: (1, 1), G: (1, 1), H: (8, 1), I: (8, 1), J: (8, 1), K: (1, 1), L: (-999, 2), M: (12, 2), N: (-999, 0), \n",
            "wednesday    \tA: (9, 1), B: (9, 1), C: (2, 1), D: (9, 1), E: (9, 1), F: (-999, 0), G: (2, 1), H: (9, 1), I: (9, 1), J: (9, 1), K: (2, 1), L: (-999, 2), M: (11, 2), N: (-999, 0), \n",
            "thursday    \tA: (10, 1), B: (10, 1), C: (-999, 0), D: (10, 1), E: (10, 1), F: (-999, 0), G: (3, 1), H: (10, 1), I: (10, 1), J: (10, 1), K: (3, 1), L: (-999, 2), M: (10, 2), N: (-999, 0), \n",
            "friday    \tA: (11, 1), B: (11, 1), C: (-999, 0), D: (11, 1), E: (11, 1), F: (-999, 0), G: (4, 1), H: (11, 1), I: (11, 1), J: (11, 1), K: (4, 1), L: (-999, 2), M: (9, 2), N: (-999, 0), \n",
            "saturday    \tA: (12, 1), B: (12, 1), C: (-999, 0), D: (12, 1), E: (12, 1), F: (-999, 0), G: (5, 1), H: (12, 1), I: (12, 1), J: (12, 1), K: (5, 1), L: (5, 2), M: (12, 2), N: (-999, 0), \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAx0K6ZauUyA"
      },
      "source": [
        "if False:\n",
        "  '''\n",
        "today is saturday\n",
        "sunday    \tA: 8, B: 8, C: -999, D: 8, E: 8, F: 1, G: 1, H: 8, I: 8, J: 8, K: 1, \n",
        "monday    \tA: 2, B: 2, C: 2, D: 2, E: 2, F: -999, G: 2, H: 2, I: 2, J: 2, K: -999, \n",
        "tuesday    \tA: 3, B: 3, C: -999, D: 3, E: 3, F: -999, G: 3, H: 3, I: 3, J: 3, K: -999, \n",
        "wednesday    \tA: 4, B: 4, C: -999, D: 4, E: 4, F: -999, G: 4, H: 4, I: 4, J: 4, K: -999, \n",
        "thursday    \tA: 5, B: 5, C: -999, D: 5, E: 5, F: -999, G: 5, H: 5, I: 5, J: 5, K: -999, \n",
        "friday    \tA: 6, B: 6, C: -999, D: 6, E: 6, F: -999, G: 6, H: 6, I: 6, J: 6, K: -999, \n",
        "saturday    \tA: 7, B: 7, C: -999, D: 7, E: 7, F: -999, G: 0, H: 7, I: 7, J: 7, K: 0, \n",
        "\ta: 7, b: 7, c: 2, d: 7, e: 7, f: 1, g: 7, h: 7, i: 7, j: 0, \n",
        "\n",
        "today is sunday\n",
        "sunday    \tA: 7, B: 7, C: -999, D: 7, E: 7, F: -999, G: 0, H: 7, I: 7, J: 7, K: 0, \n",
        "\ta: 7, b: 7, c: 2, d: 7, e: 7, f: 1, g: 7, h: 7, i: 7, j: 0, \n",
        "monday    \tA: 1, B: 1, C: -999, D: 1, E: 1, F: 1, G: 1, H: 1, I: 1, J: 1, K: -999, \n",
        "tuesday    \tA: 2, B: 2, C: 2, D: 2, E: 2, F: -999, G: 2, H: 2, I: 2, J: 2, K: -999, \n",
        "wednesday    \tA: 3, B: 3, C: -999, D: 3, E: 3, F: -999, G: 3, H: 3, I: 3, J: 3, K: -999, \n",
        "thursday    \tA: 4, B: 4, C: -999, D: 4, E: 4, F: -999, G: 4, H: 4, I: 4, J: 4, K: -999, \n",
        "friday    \tA: 5, B: 5, C: -999, D: 5, E: 5, F: -999, G: 5, H: 5, I: 5, J: 5, K: -999, \n",
        "saturday    \tA: 6, B: 6, C: -999, D: 6, E: 6, F: -999, G: 6, H: 6, I: 6, J: 6, K: -999, \n",
        "\n",
        "today is monday\n",
        "sunday    \tA: 13, B: 13, C: -999, D: 13, E: 13, F: -999, G: 6, H: 13, I: 13, J: 13, K: 6, \n",
        "monday    \tA: 7, B: 7, C: -999, D: 7, E: 7, F: -999, G: 0, H: 7, I: 7, J: 7, K: 0, \n",
        "\ta: 7, b: 7, c: 2, d: 7, e: 7, f: 1, g: 7, h: 7, i: 7, j: 0, \n",
        "tuesday    \tA: 8, B: 8, C: -999, D: 8, E: 8, F: 1, G: 1, H: 8, I: 8, J: 8, K: 1, \n",
        "wednesday    \tA: 9, B: 9, C: 2, D: 9, E: 9, F: -999, G: 2, H: 9, I: 9, J: 9, K: 2, \n",
        "thursday    \tA: 10, B: 10, C: -999, D: 10, E: 10, F: -999, G: 3, H: 10, I: 10, J: 10, K: 3, \n",
        "friday    \tA: 11, B: 11, C: -999, D: 11, E: 11, F: -999, G: 4, H: 11, I: 11, J: 11, K: 4, \n",
        "saturday    \tA: 12, B: 12, C: -999, D: 12, E: 12, F: -999, G: 5, H: 12, I: 12, J: 12, K: 5, \n",
        "  '''   "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLqojifIudRU"
      },
      "source": [
        "## check city for (POS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxZz0tJpYOM_"
      },
      "source": [
        "def check_gpe(entity, ents, ent_text, gpe, multiple_cities):\n",
        "        token = None\n",
        "        for e in ents:\n",
        "          if ent_text == e[0]:\n",
        "            token = e[1] \n",
        "            break\n",
        "        # verify the GPE is a true GPE for cases that a city has a name that has another meaninig\n",
        "        if token and token.text != token.lemma_: # a true GPE cannot be lemmatized\n",
        "          return 'continue', gpe, 0\n",
        "\n",
        "        # for example there is a city in Denemark that has a name 'rain'\n",
        "        if token and len(ents) > 1 and (token.head == token or token.pos_ not in ['PROPN', 'PRON', 'NOUN']) and not entity._.is_country:\n",
        "          valid_gpe = False\n",
        "          for e in ents:\n",
        "            if ent_text != e[0]:\n",
        "              tk = e[1]\n",
        "              if tk.head == token:  # the token is connected to another gpe, so it is a valid gpe (for example, (paris, canada) are connected\n",
        "                valid_gpe = True\n",
        "                return 'break', gpe, 0\n",
        "          if not valid_gpe:\n",
        "            if DEBUG: ic('A:', ent_text)\n",
        "            return 'continue', gpe, 0\n",
        "\n",
        "        ent_type = 'state'\n",
        "        if DEBUG: ic('0:', ent_text, entity._.is_city, entity._.is_country)\n",
        "        if entity._.is_city and not entity._.is_country: # 'canada' is a country and also a city in Portugal\n",
        "          ent_type = 'city'\n",
        "          if ent_text in df_CITIES_API.index: \n",
        "            code_country = df_CITIES_API.loc[ent_text].country\n",
        "            if type(code_country) != str:\n",
        "              multiple_cities = True\n",
        "              # see note why not to use code_country[0] which sometimes give 'Key Error'\n",
        "              # https://stackoverflow.com/questions/46153647/keyerror-0-when-accessing-value-in-pandas-series            \n",
        "              ent_country = countries_df.loc[code_country.iloc[0]].Name\n",
        "            else:\n",
        "              ent_country = countries_df.loc[code_country].Name\n",
        "          #elif ent_text in CITIES_IL:\n",
        "          #  ent_country = 'israel'\n",
        "          else:\n",
        "            ent_country = ''\n",
        "        elif entity._.is_country or ent_text in CITIES_API_country_code:\n",
        "          ent_type = 'country'\n",
        "\n",
        "        if ent_type not in gpe:\n",
        "          gpe[ent_type] = []\n",
        "        if ent_type == 'city':\n",
        "           gpe[ent_type].append((entity.text, ent_country))\n",
        "        else:\n",
        "          gpe[ent_type].append(entity.text)\n",
        "        return None, gpe, multiple_cities"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Dtx_3ng0zQ"
      },
      "source": [
        "def get_city_country(multiple_cities, gpe):\n",
        "  if DEBUG: ic('1:', multiple_cities, gpe)\n",
        "  if multiple_cities or ('city' in gpe and len(gpe['city']) > 1):\n",
        "    if 'country' in gpe:\n",
        "      # verify country code of the city is correct, when there are multiple cities\n",
        "      for i, (city, city_country) in enumerate(gpe['city']):\n",
        "        country = gpe['country'][0]\n",
        "        if city_country != country:\n",
        "          for code_country in df_CITIES_API.loc[city].country:\n",
        "            if city_country == countries_df.loc[code_country].Name:\n",
        "              # found a matching country\n",
        "              gpe['city'][i] = (city, country)\n",
        "              break\n",
        "    else:\n",
        "      check_capital = True\n",
        "      if len(gpe['city']) == 2: # maybe entered a country which has city with that name (eg. Toronto, US = there is a city name 'us' in france and 'usa' in japan)\n",
        "        # 'city': [('toronto', 'Canada'), ('us', 'france')]\n",
        "        for i in range(1,-1,-1):\n",
        "          city = gpe['city'][i][0].upper()\n",
        "          if city in countries_df.index: #usually the 2nd \"city\" is the country\n",
        "            city_country = countries_df.loc[city].Name\n",
        "            j = 0 if i == 1 else 1\n",
        "            gpe['city'] = [(gpe['city'][j][0], city_country)]\n",
        "            gpe['country'] = city_country\n",
        "            check_capital = False\n",
        "            break\n",
        "      if check_capital:\n",
        "        # check if one of the cities is a capital or a large city, then use it\n",
        "        #gpe, multiple_cities)\n",
        "        for i, (city, city_country) in enumerate(gpe['city']):\n",
        "          if city in capitals_df.index.values:\n",
        "            # found a matching country\n",
        "            city_country = capitals_df.loc[city].country\n",
        "            gpe['city'][i] = (city, city_country)\n",
        "            break\n",
        "          if city in largest_df.index.values:\n",
        "            # found a matching country\n",
        "            city_country = largest_df.loc[city].country\n",
        "            gpe['city'][i] = (city, city_country)\n",
        "            break\n",
        "  elif 'country' in gpe and 'city' in gpe:  # country with city, verify correct\n",
        "      if DEBUG: p('2:', gpe)\n",
        "      # verify country code of the city is correct, when user entered \n",
        "      city, city_country = gpe['city'][0]\n",
        "      country = gpe['country'][0]\n",
        "      if city_country != country:\n",
        "        gpe['city'] = [(city, \"ERROR: \" + MESSAGES[LANG][\"messages\"][\"city_error\"].format(city, country))]\n",
        "  elif 'country' in gpe and 'city' not in gpe:  # country without city, get the capital\n",
        "    country = gpe['country'][0]\n",
        "    #country)\n",
        "    if country in CAPITALS:\n",
        "      gpe['city'] = [(CAPITALS[country], country)]\n",
        "  return gpe"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdLpZ6kZAl4k"
      },
      "source": [
        "## get parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "ZFIUD4FCXyUc",
        "outputId": "f260edf2-aeb1-440f-9b57-3933ab13be30"
      },
      "source": [
        "DEBUG = False\n",
        "def get_parts(text, disp=False, verbose=True):\n",
        "  global DEBUG\n",
        "  doc = nlp(text.lower())\n",
        "  #doc = nlp(text.lower().translate(space_punct_dict))\n",
        "  date, time, gpe, ents = [], [], {'action':[]}, []\n",
        "  multiple_cities = False\n",
        "  if disp: p()\n",
        "  for token in doc:\n",
        "    if disp:\n",
        "      p(f\"Tokens of [{token.text}]: dep_={token.dep_}, ent_type={token.ent_type_}, head={token.head}, lemma_={token.lemma_}, pos_={token.pos_}, tag_={token.tag_}, is_action={token._.is_action}\")\n",
        "    if token._.is_action:  # checking for action first catch cities with the same name ('rain' is a city)\n",
        "      tag = ACTIONS[token.lemma_]\n",
        "      tag_exists = [t for t in gpe['action'] if t[0] == tag]\n",
        "      if len(tag_exists) == 0:\n",
        "        gpe['action'].append((tag, token.lemma_))\n",
        "    elif token.ent_type_ == 'GPE': ents.append((token.text.lower(), token))\n",
        "  for entity in doc.ents:\n",
        "      if disp:\n",
        "        p(f\"Entities of [{entity.text}]: {entity.label_}='{spacy.explain(entity.label_)}', is_city={entity._.is_city}, is_country={entity._.is_country}\")\n",
        "      ent_text = entity.text.lower().translate(remove_punct_dict)\n",
        "      if entity.label_ == 'DATE': date.append(ent_text.lower())\n",
        "      if entity.label_ == 'TIME': time.append(ent_text.lower())\n",
        "      '''\n",
        "      if entity._.is_action:  # 'rain' is both a city and weather action\n",
        "        tag = ACTIONS[ent_text]\n",
        "        gpe['action'].append((tag, ent_text))\n",
        "      '''\n",
        "      if entity.label_ == 'GPE':\n",
        "        act, gpe, multiple_cities = check_gpe(entity, ents, ent_text, gpe, multiple_cities)\n",
        "        if act == 'continue': continue\n",
        "        if act == 'break': break\n",
        "     \n",
        "  gpe = get_city_country(multiple_cities, gpe)\n",
        "    # remove city with same name as action ('rain') if there is another city\n",
        "  if 'city' in gpe:\n",
        "    org_gpe = gpe['city'].copy() # copy because the list is modified inside the loop\n",
        "    for tup in org_gpe:\n",
        "      city = tup[0]\n",
        "      city_exists = [t for t in gpe['action'] if t[0] == city or t[1] == city]\n",
        "      if len(city_exists) == 0: continue\n",
        "      if len(gpe['city']) == 1:  # must leave at least one city\n",
        "        # remove from action list if appears only once in the sentence\n",
        "        n = [t for t in doc if t.text == city]\n",
        "        if len(n) > 1: continue\n",
        "        gpe['action'].remove(city_exists[0])\n",
        "        continue\n",
        "      gpe['city'].remove(tup)\n",
        "\n",
        "#  if len(date) == 0:\n",
        "#    date = 'current'\n",
        "  if len(gpe['action']) > 0:\n",
        "    #gpe['action'] = [('all', 'weather')]\n",
        "    #else:\n",
        "    for i, tup in enumerate(gpe['action']):\n",
        "      if tup[0] == 'all':\n",
        "        if i == 0: break\n",
        "        gpe['action'].remove(tup)\n",
        "        gpe['action'].insert(0, tup)\n",
        "        break\n",
        "\n",
        "  result = {'date':date, 'time':time}\n",
        "  result.update(gpe)\n",
        "\n",
        "  if disp:\n",
        "    p(result)\n",
        "  elif verbose and 'city' not in result:\n",
        "    p(f'FAILED: org [{text}], {result}')\n",
        "  return result\n",
        "\n",
        "#DEBUG = False\n",
        "if TEST:\n",
        "  get_parts(\"Today weather in a Ra-anana\")\n",
        "  get_parts(\"Today weather in a Ra'anana\")  \n",
        "  get_parts(\"Today weather in a Raanana\")\n",
        "  get_parts(\"weather in toronto\") # there is a city toronto in USA and toronto is not the capital of Canada so we take the first city in the db\n",
        "  get_parts(\"weather in toronto, canada\")\n",
        "  get_parts(\"weather paris next 5 days\")\n",
        "  #get_parts(\"tell me how is the weather in the ra- anana in the\")\n",
        "  get_parts(\"rain in toronto\")\n",
        "  get_parts(\"weather in toronto, us\")\n",
        "  get_parts(\"weather in toronto, canada\")\n",
        "  get_parts(\"weather in jenin\")\n",
        "  get_parts(\"weather in jenin, israel\")\n",
        "  get_parts('haifa, israel')\n",
        "  DEBUG = False\n",
        "  get_parts(\"Wheather forecast in Tel aviv\")\n",
        "  get_parts(\"paris Canada\")\n",
        "  get_parts(\"heat in paris\")\n",
        "  get_parts(\"clouds, heat, rain in toronto\")\n",
        "  get_parts(\"clouds, heat, rain, temp, pressure, humidity, wind, visibility, sunrise in toronto\")\n",
        "  get_parts(\"clouds, heat and rain in rain\")\n",
        "#answer = get_weather('current', [('jerusalem', 'israel')], [('clouds', 'cloud'), ('temp', 'temp'), ('pressure', 'pressure'), ('humidity', 'humidity'), ('wind', 'wind'), ('visibility', 'wind'), ('sunrise', 'wind'), ('weather','rain')])\n",
        "#answer = get_weather('current', [('jerusalem', 'israel')], [('clouds', 'cloud'), ('temp', 'temp'), ('pressure', 'pressure'), ('humidity', 'humidity'), ('wind', 'wind'), ('visibility', 'wind'), ('sunrise', 'wind'), ('weather','rain')])\n",
        "#answer\n",
        "  get_parts(\"rain, clouds, weather in toronto, canada\", True)\n",
        "  get_parts(\"rain, clouds, weather in toronto\", True)\n",
        "\n",
        "  l = get_parts('in one week')['date']\n",
        "  p(l, check_date(l))\n",
        "  l = get_parts(\"friday in one week\", True)['date']\n",
        "  p(l, check_date(l))\n",
        "  l = get_parts(\"friday in 1 week\", True)['date']\n",
        "  p(l, check_date(l))\n",
        "  l = get_parts(\"in 1 week\", True)['date']\n",
        "  p(l, check_date(l))\n",
        "  # not recognized by spacy: now, current, right now \n",
        "get_parts(\"weather week in tel aviv\", True)  "
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokens of [weather]: dep_=compound, ent_type=DATE, head=week, lemma_=weather, pos_=NOUN, tag_=NN, is_action=True\n",
            "Tokens of [week]: dep_=ROOT, ent_type=DATE, head=week, lemma_=week, pos_=NOUN, tag_=NN, is_action=False\n",
            "Tokens of [in]: dep_=prep, ent_type=, head=week, lemma_=in, pos_=ADP, tag_=IN, is_action=False\n",
            "Tokens of [tel]: dep_=compound, ent_type=GPE, head=aviv, lemma_=tel, pos_=PROPN, tag_=NNP, is_action=False\n",
            "Tokens of [aviv]: dep_=pobj, ent_type=GPE, head=in, lemma_=aviv, pos_=PROPN, tag_=NNP, is_action=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk\\nimport bokeh\\nimport fbprophet\\nimport pydot\\nfrom sklearn.model_selection import cross_val_score\\nimport pandas as pd\\nfrom xlrd import open_workbook\\nimport os\\nimport plotly as py\\nimport pickle\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport numpy as np\\nimport altair as alt\\nfrom sklearn.manifold import TSNE\\nimport imutils\\nimport spacy\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport re\\nimport fastai\\nfrom statsmodels.tsa.arima_model import ARIMA\\nimport plotly.express as px\\nimport matplotlib.pyplot as plt\\nimport cv2'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [weather week]: DATE='Absolute or relative dates or periods', is_city=False, is_country=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk\\nimport bokeh\\nimport fbprophet\\nimport pydot\\nfrom sklearn.model_selection import cross_val_score\\nimport pandas as pd\\nfrom xlrd import open_workbook\\nimport os\\nimport plotly as py\\nimport pickle\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport numpy as np\\nimport altair as alt\\nfrom sklearn.manifold import TSNE\\nimport imutils\\nimport spacy\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport re\\nimport fastai\\nfrom statsmodels.tsa.arima_model import ARIMA\\nimport plotly.express as px\\nimport matplotlib.pyplot as plt\\nimport cv2'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [tel aviv]: GPE='Countries, cities, states', is_city=True, is_country=False\n",
            "{'date': ['weather week'], 'time': [], 'action': [('all', 'weather')], 'city': [('tel aviv', 'israel')]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action': [('all', 'weather')],\n",
              " 'city': [('tel aviv', 'israel')],\n",
              " 'date': ['weather week'],\n",
              " 'time': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "RZa9eQ4RPPF_",
        "outputId": "69369674-64f2-4de5-f65c-cb34c64f8caf"
      },
      "source": [
        "get_parts(\"20/10/2021\", True)\n",
        "get_parts(\"20/10/2021\", True)\n",
        "get_parts(\"20 10 2021\", True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokens of [20/10/2021]: dep_=ROOT, ent_type=, head=20/10/2021, lemma_=20/10/2021, pos_=NUM, tag_=CD, is_action=False\n",
            "{'date': 'current', 'time': [], 'action': []}\n",
            "\n",
            "Tokens of [20/10/2021]: dep_=ROOT, ent_type=, head=20/10/2021, lemma_=20/10/2021, pos_=NUM, tag_=CD, is_action=False\n",
            "{'date': 'current', 'time': [], 'action': []}\n",
            "\n",
            "Tokens of [20]: dep_=ROOT, ent_type=DATE, head=20, lemma_=20, pos_=NUM, tag_=CD, is_action=False\n",
            "Tokens of [10]: dep_=nummod, ent_type=DATE, head=2021, lemma_=10, pos_=NUM, tag_=CD, is_action=False\n",
            "Tokens of [2021]: dep_=quantmod, ent_type=DATE, head=20, lemma_=2021, pos_=NUM, tag_=CD, is_action=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [20 10 2021]: DATE='Absolute or relative dates or periods', is_city=False, is_country=False\n",
            "{'date': ['20 10 2021'], 'time': [], 'action': []}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action': [], 'date': ['20 10 2021'], 'time': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zapNCcw0yq4",
        "outputId": "d8029a1e-4838-4154-9b06-d826203a7d84"
      },
      "source": [
        "if TEST:\n",
        "  get_parts(\"paris Canada\")\n",
        "  get_parts(\"las vegas, united states\")\n",
        "  get_parts(\"las vegas, US\")  \n",
        "  get_parts(\"weather in las vegas, united states\")\n",
        "  get_parts(\"weather in las vegas, US\")\n",
        "  get_parts(\"weather in new york\")    \n",
        "  get_parts(\"weather in new york us\")\n",
        "get_parts(\"paris, Canada\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FAILED: org [paris, Canada], {'date': 'current', 'time': [], 'action': []}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action': [], 'date': 'current', 'time': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQv631Mpz7Xw"
      },
      "source": [
        "DEBUG = False\n",
        "if TEST:\n",
        "  get_parts(\"please tell me if it will rain tonight in Rain\")\n",
        "  get_parts(\"tomorrow and today rain in spain\")\n",
        "  get_parts(\"tomorrow rain in madrid\")\n",
        "  get_parts(\"tomorrow rain in beersheba\")\n",
        "  get_parts(\"tomorrow rain in israel\")\n",
        "  get_parts(\"weather paris Canada\")\n",
        "  get_parts(\"weather paris, Canada\")\n",
        "  get_parts(\"tomorrow rain in yafo\")\n",
        "  "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "v4aWemRoAIUf",
        "outputId": "ba1ef202-108a-4ae3-b0be-4d7b7f4158fc"
      },
      "source": [
        "if TEST:\n",
        "  get_parts(\"weather in new york\")\n",
        "  get_parts(\"weather in new york us\")\n",
        "  get_parts(\"weather in las vegas\")\n",
        "  get_parts(\"weather in las vegas, US\")\n",
        "  get_parts(\"weather in las vegas, united states\")\n",
        "  get_parts(\"weather in kas vegas\")\n",
        "  get_parts(\"weather in kas vegas, US\")\n",
        "  get_parts(\"what is the weather in summer, Israel\")\n",
        "# ask before switching a langauge\n",
        "get_parts(\"weather in las vegas, us\", True)\n",
        "get_parts(\"weather in new england, us\", True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokens of [weather]: dep_=ROOT, ent_type=, head=weather, lemma_=weather, pos_=NOUN, tag_=NN, is_action=True\n",
            "Tokens of [in]: dep_=prep, ent_type=, head=weather, lemma_=in, pos_=ADP, tag_=IN, is_action=False\n",
            "Tokens of [las]: dep_=compound, ent_type=GPE, head=vegas, lemma_=las, pos_=PROPN, tag_=NNP, is_action=False\n",
            "Tokens of [vegas]: dep_=pobj, ent_type=GPE, head=in, lemma_=vegas, pos_=PROPN, tag_=NNP, is_action=False\n",
            "Tokens of [,]: dep_=punct, ent_type=, head=weather, lemma_=,, pos_=PUNCT, tag_=,, is_action=False\n",
            "Tokens of [us]: dep_=npadvmod, ent_type=GPE, head=weather, lemma_=us, pos_=PROPN, tag_=NNP, is_action=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [las vegas]: GPE='Countries, cities, states', is_city=True, is_country=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [us]: GPE='Countries, cities, states', is_city=True, is_country=False\n",
            "{'date': 'current', 'time': [], 'action': [('all', 'weather')], 'city': [('las vegas', 'united states')], 'country': 'united states'}\n",
            "\n",
            "Tokens of [weather]: dep_=ROOT, ent_type=, head=weather, lemma_=weather, pos_=NOUN, tag_=NN, is_action=True\n",
            "Tokens of [in]: dep_=prep, ent_type=, head=weather, lemma_=in, pos_=ADP, tag_=IN, is_action=False\n",
            "Tokens of [new]: dep_=compound, ent_type=, head=england, lemma_=new, pos_=PROPN, tag_=NNP, is_action=False\n",
            "Tokens of [england]: dep_=pobj, ent_type=GPE, head=in, lemma_=england, pos_=PROPN, tag_=NNP, is_action=False\n",
            "Tokens of [,]: dep_=punct, ent_type=, head=england, lemma_=,, pos_=PUNCT, tag_=,, is_action=False\n",
            "Tokens of [us]: dep_=pobj, ent_type=GPE, head=in, lemma_=us, pos_=PROPN, tag_=NNP, is_action=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [england]: GPE='Countries, cities, states', is_city=True, is_country=False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import spacy\\nimport nltk\\nimport pandas as pd'); }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Entities of [us]: GPE='Countries, cities, states', is_city=True, is_country=False\n",
            "{'date': 'current', 'time': [], 'action': [('all', 'weather')], 'city': [('england', 'united states')], 'country': 'united states'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action': [('all', 'weather')],\n",
              " 'city': [('england', 'united states')],\n",
              " 'country': 'united states',\n",
              " 'date': 'current',\n",
              " 'time': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcfVvMeRddoX"
      },
      "source": [
        "if TEST:\n",
        "  get_parts(\"Today weather in a Ra-anana\")\n",
        "  get_parts(\"weather in tel aviv\")\n",
        "  get_parts(\"tomorrow weather in haifa\")\n",
        "  get_parts(\"tomorrow rain in beersheba\")\n",
        "  get_parts(\"weather paris today and tomorrow\")\n",
        "  get_parts(\"weather paris\")\n",
        "  get_parts(\"weather paris in December\")\n",
        "  get_parts(\"ra'anana\")\n",
        "  get_parts(\"Today weather in a Tel Aviv\")\n",
        "  get_parts(\"weather paris\")\n",
        "  get_parts(\"paris weather \")\n",
        "  get_parts('Paris weather in')\n",
        "  get_parts(\"weather shelomi\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAr4WaLVDL2x"
      },
      "source": [
        "if TEST:\n",
        "  # example of successful chat although get parts failed\n",
        "  get_parts(\"weather in tel-aviv\")\n",
        "  get_parts(\"weather in TelAviv\")   # gave answer for Israel/Jerusalem\n",
        "  get_parts(\"what is tel aviv\")\n",
        "  get_parts(\"wether in tel aviv\")\n",
        "  get_parts(\"please tell me if it will rain tonight in Rain\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAH-c7izF8YF"
      },
      "source": [
        "if TEST:\n",
        "  # fixed in round1 of the bot (remove stop words and dashes followed by a space '- ')\n",
        "  get_parts(\"weather ra- anana\") # for google\n",
        "  get_parts(\"tell me how is the weather in the ra- anana in the\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtB-Itaok9Br"
      },
      "source": [
        "# Run the bot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2lBe7UTUhfO"
      },
      "source": [
        "> when similarity was .075 it matched the sentence\n",
        "\n",
        ">\"tell me how is the weather in the ra- anana in the\"\n",
        "\n",
        ">to \"what's up\" of tag [\"greeting\"] so we increased it to 0.85"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6T_HYP7seLq"
      },
      "source": [
        "## Google search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ry8ASR8NEdKi",
        "outputId": "ba053fa5-35ff-4a3b-e69c-be4ba4d463ab"
      },
      "source": [
        "def get_google(text):\n",
        "  url = \"https://google.com/search?q=\" + text + '&hl=en'\n",
        "  request_result = requests.get(url)\n",
        "  soup = bs4.BeautifulSoup(request_result.text, \"html.parser\")\n",
        "  spans = soup.find_all('span', class_='BNeawe')\n",
        "  new_text = ''\n",
        "  for span in spans: \n",
        "    if isinstance(span.contents[0], bs4.NavigableString):\n",
        "      new_text += str(span.contents[0]) + ' '\n",
        "  return new_text\n",
        "get_google(\"'קשאיקר ןמ ןדרשקך\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tel Aviv-Yafo, Israel  /  Weather '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4AnUiGxRt1z"
      },
      "source": [
        "def add_language(lang):\n",
        "  lang_exists = [t for t in MESSAGES_lang if t == lang]\n",
        "  if len(lang_exists) > 0:\n",
        "    p(f'{lang} already exists')\n",
        "    return\n",
        "  MESSAGES_lang.append(lang)\n",
        "  p(MESSAGES_lang)\n",
        "  MESSAGES.update({lang:{'messages':{}}})\n",
        "  new_msg = list(translator.translate(list(MESSAGES_en.values()), dest=lang))\n",
        "  for i, k in enumerate(MESSAGES_en.keys()):\n",
        "    if type(new_msg[i]) is list:\n",
        "      for msg in new_msg[i]:\n",
        "        MESSAGES[lang][\"messages\"].update({k: msg.text})\n",
        "      continue\n",
        "    MESSAGES[lang][\"messages\"].update({k: new_msg[i].text})\n",
        "#  MESSAGES[lang])\n",
        "  new_msg = translator.translate(list(intents_en), dest=lang)\n",
        "  intents.update({lang:{'messages':[]}})\n",
        "  msg_arr, i = [], 0\n",
        "  for dict_ in intents['en']['messages']:\n",
        "    resp, msg = [], {}\n",
        "    for k, v in dict_.items():\n",
        "      if k == 'tag' or k == 'patterns':\n",
        "        msg.update({k: v})\n",
        "        continue\n",
        "      for j in range(len(intents['en'][\"messages\"][i][\"responses\"])):\n",
        "        resp.append(new_msg[i][j].text)\n",
        "    msg.update({\"responses\": resp})\n",
        "    msg_arr.append(msg)    \n",
        "    i += 1\n",
        "  intents[lang].update({\"messages\": msg_arr})\n",
        "  # save the new langauge\n",
        "  new_d = {\"intents\": {\"language\": {lang: {'messages': {}}}},\n",
        "          \"messages\": {\"language\": {lang: {'messages': {}}}}}\n",
        "  new_d['intents']['language'][lang] = intents[lang]\n",
        "  new_d['messages']['language'][lang] = MESSAGES[lang]\n",
        "  # read old new langaused that were not yet added to main messages file\n",
        "  list_d = []\n",
        "  try:\n",
        "    with open(path + 'new_messages.json', \"r\") as f:\n",
        "      list_d = json.loads(f.read())\n",
        "  except Exception as error:\n",
        "    pass\n",
        "    #error)\n",
        "  list_d.insert(0, new_d) #append(new_d)\n",
        "  #list_d)\n",
        "  with open(path + 'new_messages.json', \"w\") as f:\n",
        "    json.dump(list_d, f, indent=1, ensure_ascii = False)\n",
        "    \n",
        "#translator.detect('الشغل'))\n",
        "\n",
        "# 'xab' is in Hmong langauge (hmn) \n",
        "#translator.detect('Работа'))\n",
        "#add_language('iw') # hebrew\n",
        "#add_language('ar') # arabic\n",
        "#add_language('ru') # russian\n",
        "#add_language('fr')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkw46AjN1dpL"
      },
      "source": [
        "## Send email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4imYrVOXh0hO"
      },
      "source": [
        "SMTPAuthenticationError: (534, b'5.7.14 <https://accounts.google.com/signin/continue?sarp=1&scc=1&plt=AKgnsbu\\n5.7.14 wikqxaraOBaoJ8uaTDqCVoXmzPp36d_A6D8Ay2dQ-cRIGSVS1O88IW5d5hVoC21k64yZ8\\n5.7.14 zt8t4BLQx1zO_S0arOXBdjPX4pd3KRLEhWoYyXJorkcAkitmYm1SAhepjia4Eep4>\\n5.7.14 Please log in via your web browser and then try again.\\n5.7.14  Learn more at\\n5.7.14  https://support.google.com/mail/answer/78754 m13sm2830102vsl.16 - gsmtp')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHRsqiS908zK"
      },
      "source": [
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "msg = EmailMessage()\n",
        "\n",
        "# locally, if fails to send open google 'unlock captcha' and activate it https://accounts.google.com/b/0/DisplayUnlockCaptcha\n",
        "\n",
        "def send_email(text='Mmessage from WeatherBot local', subject='New Session', to=\"drbarak@talkie.co.il\"):\n",
        "  msg = EmailMessage()\n",
        "\n",
        "  msg.set_content('')\n",
        "  # Add the html version.  This converts the message into a multipart/alternative\n",
        "  # container, with the original text message as the first part and the new html\n",
        "  # message as the second part. (Note that the first part of the messge did not work for me and didnot displaed on the incomin email)\n",
        "  msg.add_alternative('''\n",
        "  <html>\n",
        "    <head>\n",
        "      <h1>{subject}</h1>\n",
        "    </head>\n",
        "    <body>\n",
        "      {text}\n",
        "    </body>\n",
        "  </html>\n",
        "  '''.format(subject=subject, text=text), subtype='html')\n",
        "  msg['Subject'] = subject\n",
        "  msg['From'] = \"Weatherbot <dr.zvibarak@gmail.com>\"\n",
        "  msg['To'] = to\n",
        "\n",
        "  try:\n",
        "    smtpObj = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "    smtpObj.starttls()\n",
        "    smtpObj.login('dr.zvibarak@gmail.com', 'shushu1952')\n",
        "    smtpObj.send_message(msg)\n",
        "    smtpObj.quit()\n",
        "    print(\"Successfully sent email\")\n",
        "    return True\n",
        "  except Exception as e:# SMTPException:\n",
        "    print(\"Error: unable to send email\")\n",
        "    p(e)\n",
        "    return False\n",
        "#send_email(text=\"user_msg\", subject='Log from WeatherBot', to=\"drbarak@talkie.co.il\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry23BUN9weHj",
        "outputId": "026184e0-4847-4d6b-b7a7-56bdd645099b"
      },
      "source": [
        "GOOGLE = True\n",
        "do = True\n",
        "GREETING, NOANSWER = 0, 3\n",
        "#intents['intents'][GREETING]['responses'][0])\n",
        "min_similarity = 0.84\n",
        "VERBOSE = 0\n",
        "LANG = 'en'\n",
        "new_lang = ''\n",
        "RUN_TEST = 0\n",
        "\n",
        "while do:\n",
        "\n",
        "  if RUN_TEST > 0:\n",
        "    user_msg = QUESTIONS[RUN_TEST- 1]\n",
        "    p(user_msg)\n",
        "    RUN_TEST += 1\n",
        "    VERBOSE = 0\n",
        "    if RUN_TEST > len(QUESTIONS): RUN_TEST = 0\n",
        "  else:\n",
        "    user_msg = input().lower().strip()\n",
        "  if user_msg == '':\n",
        "    answers = intents[LANG][\"messages\"][NOANSWER]['responses']\n",
        "    p(\"WeatherBot: \" + random.choice(answers))\n",
        "    continue\n",
        "  if user_msg == 'run test':\n",
        "    RUN_TEST = 1\n",
        "    continue\n",
        "\n",
        "  switched_lang = False\n",
        "  if new_lang != '':\n",
        "    if user_msg == '1':\n",
        "      if new_lang not in MESSAGES_lang or new_lang=='fr':\n",
        "        add_language(new_lang)\n",
        "      LANG = new_lang\n",
        "      switched_lang = True\n",
        "    user_msg = org_msg\n",
        "    new_lang = ''\n",
        "  elif not RUN_TEST:\n",
        "    response = translator.detect(user_msg)\n",
        "    if response.lang == 'iw': response.lang = 'he'\n",
        "    if response.lang != LANG:\n",
        "      p(response.lang)\n",
        "      if response.lang in ['en','he']:\n",
        "        LANG = response.lang\n",
        "        switched_lang = True\n",
        "      else:\n",
        "        verify = True\n",
        "        if response.lang not in MESSAGES_lang:\n",
        "            # add langauge only when sure - of at least 3 words of length 5 letters each - to prevent erroneous detection\n",
        "          sentence = ' '.join(filter(None, user_msg.split(' ')))\n",
        "          if not (len(sentence) > 11 and len(user_msg.split()) > 2):\n",
        "            verify = False  # False \"new langauge\" - google think that 'haifa israel' is in arabic, and 'Paris, frnace' is in french\n",
        "        if verify:\n",
        "          new_lang = response.lang\n",
        "          org_msg = user_msg\n",
        "          p(\"WeatherBot: \" + MESSAGES[LANG][\"messages\"][\"new_lang\"].format(new_lang))\n",
        "          continue\n",
        "    elif LANG == 'en': # check if the google translate stopped working\n",
        "      test = translator.detect('שלום')\n",
        "      if test.lang == LANG:\n",
        "        #p('Note: Google translate stopped working temporarily -> multi-lingual option is not functional')\n",
        "        pass\n",
        "\n",
        "  round = 0\n",
        "  answer = ''\n",
        "  org_msg = user_msg\n",
        "  tag = ''\n",
        "  while round < 5 and do:\n",
        "    if VERBOSE: p(f'round {round}: {user_msg}')\n",
        "    doc1 = nlp(user_msg)\n",
        "    answers, tag = None, None\n",
        "    top_similarity = 0\n",
        "    # verify not an erroneous finding (if we have GPE in the doc we assume it is not a general request)\n",
        "    '''\n",
        "    for token in doc1:\n",
        "      p(f\"Tokens of [{token.text}]: dep_={token.dep_}, ent_type={token.ent_type_}, head={token.head}, lemma_={token.lemma_}, pos_={token.pos_}, tag_={token.tag_}, is_action={token._.is_action}, like_email ={token.like_email }\")\n",
        "    for entity in doc1.ents:\n",
        "      p(f\"Entities of [{entity.text}]: {entity.label_}='{spacy.explain(entity.label_)}', is_city={entity._.is_city}, is_country={entity._.is_country}\")\n",
        "    '''\n",
        "    if len([ent for ent in doc1.ents if ent.label_ == 'GPE']) == 0 or len([tk for tk in doc1 if tk.like_email]) > 0:\n",
        "      for ints in intents[LANG][\"messages\"]:\n",
        "        #ints)\n",
        "        for text in ints['patterns']:\n",
        "          doc2 = nlp(text)\n",
        "          # Get the similarity of doc1 and doc2\n",
        "          if doc1.vector_norm == 0 or doc2.vector_norm == 0:\n",
        "            continue\n",
        "          similarity = doc1.similarity(doc2)\n",
        "          if VERBOSE > 2: p(f\"{similarity}, {doc2}, {[ints['tag']]}\")\n",
        "          if similarity > top_similarity:\n",
        "            top_similarity = similarity\n",
        "            answers = ints['responses']\n",
        "            tag = ints['tag']\n",
        "      #p(top_similarity, tag)\n",
        "      #p(answers, tag)\n",
        "      shift = 3 if tag == \"mail\" else 0\n",
        "      if top_similarity > min_similarity - shift:\n",
        "        if tag == \"goodbye\":\n",
        "          do = False\n",
        "        if tag in [\"goodbye\", \"greeting\", \"thanks\", \"options\"]:\n",
        "          answer = random.choice(answers)\n",
        "          break\n",
        "        if tag == \"mail\":\n",
        "          to = [tk for tk in doc1 if tk.like_email][0]\n",
        "          if not send_email(text=user_msg, subject='Log from WeatherBot (local)', to=to):\n",
        "            answer = f\"Failed to send email to [{to}]\"\n",
        "          else:\n",
        "            answer = random.choice(answers)\n",
        "          break\n",
        "    if round == 2 and GOOGLE: round = 3 # to know we got the answer from get_parts()\n",
        "    parts = get_parts(user_msg, VERBOSE > 1, False)\n",
        "    if VERBOSE:  p('x:', parts)\n",
        "    days = NO_DAYS\n",
        "    if 'date' in parts:\n",
        "      days, range_days = check_date(parts['date'])\n",
        "      if True or VERBOSE:  p(days, range_days)\n",
        "      if days > 7: days = NO_DAYS\n",
        "      else:\n",
        "        if days + range_days > 8:\n",
        "          range_days = 7 - days\n",
        "        if range_days == 0: range_days = 1\n",
        "        if True or VERBOSE:  p(days, range_days)\n",
        "    if days != NO_DAYS and 'city' in parts and 'ERROR' not in parts['city'][0][1] and len(parts['action']) > 0:\n",
        "      p(round, parts)\n",
        "      answer, url_icon = get_weather(days, parts['city'], parts['action'], range_days, LANG)\n",
        "      break\n",
        "    elif 'city' in parts and 'ERROR' in parts['city'][0][1] and not switched_lang:  #to try google for gibrish\n",
        "      answer = parts['city'][0][1]\n",
        "      break\n",
        "    elif round == 0:\n",
        "      round = 1\n",
        "      user_msg = ''\n",
        "      space = ' '\n",
        "      for token in doc1:\n",
        "        if token.text in stopwords: continue\n",
        "        text = token.text.lower().translate(remove_punct_dict)\n",
        "        if text == '': continue\n",
        "        if text[-1] == '-': # take care of 'ra- anana'\n",
        "          user_msg += space + text[:-1]\n",
        "          space = ''\n",
        "        else:\n",
        "          user_msg += space + text\n",
        "          space = ' '\n",
        "      continue\n",
        "    elif round == 1:\n",
        "      round = 2\n",
        "      user_msg = translator.translate(org_msg) # default translation is to english - can set source/dest langauge: src=\"de\"/ dest=\"he\"\n",
        "      if VERBOSE:  p(f\"{user_msg.origin} ({user_msg.src}) --> {user_msg.text} ({user_msg.dest})\")\n",
        "      user_msg = user_msg.text\n",
        "      if user_msg != '':\n",
        "        continue\n",
        "    if round == 2 or round == 3:  # need this round because in round 2 get_google('ביי') returns empty string\n",
        "      round = 4\n",
        "      user_msg = get_google(org_msg)\n",
        "      #ic(round, switched_lang, user_msg, org_msg)\n",
        "      if user_msg != '':\n",
        "        continue\n",
        "    if round > 3:\n",
        "      break\n",
        "  if round >= 3 and not answer == '' and not 'ERROR' in answer:\n",
        "    #ic(type(answer))\n",
        "    #ic(answer)\n",
        "    answer += f' ({MESSAGES[LANG][\"messages\"][\"from_google\"]})'\n",
        "  if answer == '':\n",
        "    if 'city' not in parts or len(parts['city']) == 0:\n",
        "      answer = MESSAGES[LANG][\"messages\"][\"city_unknown\"]  \n",
        "    elif 'action' not in parts or len(parts['action']) == 0:\n",
        "      answer = MESSAGES[LANG][\"messages\"][\"action_unknown\"] \n",
        "    else: # if 'date' not in parts or len(parts['date']) == 0:\n",
        "      #answer = MESSAGES[LANG][\"messages\"][\"date_unknown\"] \n",
        "      answer = \"You need to tell me for when do you want the weather information you are looking for (such as 'today', 'tomorrow', 'monday'), up to 7 days from today\"\n",
        "  p(f\"WeatherBot: {answer}\")\n",
        "  if RUN_TEST:\n",
        "    p()\n",
        "  #ic(tag)\n",
        "# weather in jenin, israel  \n",
        "# 'קשאיקר ןמ ןדרשקך (weather in israel in hebrew)\n",
        "# clouds and pressure in toronto\n",
        "# clouds, temp, pressure, humidity in toronto\n",
        "# get_parts(\"clouds, heat, rain, temp, pressure, humidity, wind, visibility, sunrise in toronto\")\n",
        "# get_parts(\"clouds, heat and rain in rain\", True)\n",
        "# send drbarak@talkie.co.il\n",
        "# rain in toronto tomorrow\n"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rain in toronto tomorrow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ic| <ipython-input-223-4b091e598a7f>:15 in call_web()\n",
            "    api_url: 'http://api.openweathermap.org/data/2.5/onecall?lat=43.700111&lon=-79.416298&exclude=minutely,hourly&units=metric&lang=en&appid=c2adfa29edfd95ad16efab9218619ff3'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x: {'date': ['tomorrow'], 'time': [], 'action': [('weather', 'rain')], 'city': [('toronto', 'canada')]}\n",
            "1 1\n",
            "1 1\n",
            "0 {'date': ['tomorrow'], 'time': [], 'action': [('weather', 'rain')], 'city': [('toronto', 'canada')]}\n",
            "True\n",
            "WeatherBot: In Toronto, Canada the current weather is: <br>The weather tomorrow:  no rain, broken clouds,<br>The weather in 2 days:  no rain, broken clouds,<br>The weather in 3 days:  no rain, broken clouds,<br>The weather in 4 days:  no rain, broken clouds,<br>The weather in 5 days:  no rain, broken clouds,<br>The weather in 6 days:  no rain, broken clouds,<br>The weather in 7 days:  no rain, broken clouds,<br>The weather in 8 days:  no rain, broken clouds\n",
            "rain in toronto now\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ic| <ipython-input-223-4b091e598a7f>:15 in call_web()\n",
            "    api_url: 'http://api.openweathermap.org/data/2.5/weather?q=toronto,,CA&units=metric&lang=en&appid=c2adfa29edfd95ad16efab9218619ff3'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x: {'date': ['now'], 'time': [], 'action': [('weather', 'rain')], 'city': [('toronto', 'canada')]}\n",
            "0 1\n",
            "0 1\n",
            "0 {'date': ['now'], 'time': [], 'action': [('weather', 'rain')], 'city': [('toronto', 'canada')]}\n",
            "False\n",
            "WeatherBot: In Toronto, Canada the current weather is:  no rain, broken clouds\n",
            "bye\n",
            "WeatherBot: See you!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwK8AS5vcuCq"
      },
      "source": [
        "# TODO:\n",
        "\n",
        "1. DONE: display country when getting result of weather so the user knows it is the right country - if there multiple cities\n",
        "\n",
        "2. DONE: load jsons from github - (one file too large so download it from a shared goole link on my google drive)\n",
        "\n",
        "3. DONE: Use flask\n",
        "\n",
        "4. DONE: if fails to understand try round 2 - removing stop words (punctuation akready done by spacy), then round 3 use google to try to fix typo mistake and only if all fails display a message to user\n",
        "\n",
        "5. To identify if it is a question or not - must be question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKZtAV4zK0Ud"
      },
      "source": [
        "# QUESTION:\n",
        "\n",
        "2. Are we to handle 'Ra-anana'? we handle 'Raanana'\n",
        "3. DONE - There is a city toronto in USA and toronto is not the capital of Canada so we take the first city in the db (we check now if it is one of the 100 largest cities in the world)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWIFDxfteOe"
      },
      "source": [
        "# EXAMPLES\n",
        "\n",
        "0 You:\tweather in jenin, israel\n",
        "\n",
        "1\tBot:\tERROR: city [jenin] not found in country [israel] - try a different spelling of the city\n",
        "\n",
        "2\tYou:\tweather in hebron, israel\n",
        "\n",
        "3\tBot:\tIn Hebron, Israel the current weather is: clear sky\n",
        "\n",
        "4\tYou:\tweather in jenin\n",
        "\n",
        "5\tBot:\tIn Jenin, Poland the current weather is: overcast clouds\n",
        "\n",
        "6\tYou:\tweather in hebron\n",
        "\n",
        "7\tBot:\tIn Hebron, Palestine, state of the current weather is: clear sky\n",
        "\n",
        "8 weather isral\n",
        "\n",
        "9 WeatherBot: In Jerusalem, Israel the current weather is: clear sky (from Google)\n",
        "\n",
        "10 中國的天氣\n",
        "\n",
        "In Beijing, China the current weather is: overcast clouds (from Google)\n",
        "\n",
        "11 погода в китае\n",
        "12 погода в россии"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nki5uNqizBRD"
      },
      "source": [
        "# TEST"
      ]
    }
  ]
}